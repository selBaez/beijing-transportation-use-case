preparation						cleaning																																							DONE
											information extraction (mode,line,stop,#trips,time bins)															DONE
											patching																																							DONE
											formatting																																						DONE

preprocessing					label data																																						DONE
											standardize																																						Refactor parameters
											create user cubes/vectors																															DONE

feature selection 		preprocessing -> label data																														DONE
											correlation tests (correlation, feature importance, chi2 and anova f value)						DONE
											preprocessing -> standardize
											select k best
											preprocessing -> create user cubes
											preprocessing -> flatten cubes into vectors
											save vectors

ensemble classifier 	create models (SVM, bayesian, random forest, mlp)
											train using K-fold with replacement
											ensemble (adaboost or bagging)
											test
											save model
											predict
											save card codes and their labels

feature engineering 	preprocessing -> standardize
											preprocessing -> create user cubes
											create convolutional autoencoder																												Needs to be tested
											train encoder																																						Needs to be tested
											encode samples																																					Needs to be tested
											visualize low dimensional data																													Needs to be tested
											save data																																								DONE

clustering 						k-means
											cluster analysis (silhouettes samples for cluster membership)


TODO:
tsne plots for these

normalize areas? keep as ordinal?
num of transfer as pie chart
fix start/end hour 0. change to 24?

BIG ISSUE: vocabularies grow!!
FIX ASAP: new lines in voc


NOTE:
currently transforming data over whole data set and not over train/test or cross validation.
 	pro: convenient because normalization is over type of feature, meaning 'TRAVEL TIME' over all trips BEFORE merging per user
				to change it would mean to grab feature slices from each cube, instead of values from each vector
				list of slices, instead of list of values (vector)
	con: scientifically correct? travel time and distance has large tail, thus the distribution really depends on the data

Papers to get:
Reconstructing individual mobility from smart card transactions: a collaborative space alignment approach

Improvements to report:
transfer time focus on tail
thesis organization more technical, and methods per parts
Mention hierarchy in traffic areas will cause correlation
Attributes summary: numerical vs categorical
Section: deal with categorical -> tokenize, encode, bins
comment on ordinal structure vs one hot
random forests deal well with categorical (large node size), adaboost with missing info

categorize mode, stop, line: zero cannot be used cause it means empty.

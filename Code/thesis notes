preparation						cleaning																																							DONE
											information extraction (mode,line,stop,#trips,time bins)															Extend vocabulary
											patching																																							DONE
											formatting																																						DONE

preprocessing					label data																																						DONE
											standardize																																						Refactor parameters
											create user cubes/vectors																															DONE

feature selection 		preprocessing -> label data																														DONE
											correlation tests (correlation, feature importance, chi2 and anova f value)
											select k best
											preprocessing -> standardize
											preprocessing -> create user cubes
											flatten cubes into vectors
											save vectors

ensemble classifier 	create models (SVM, bayesian, random forest, mlp)
											train using K-fold with replacement
											ensemble (adaboost or bagging)
											test
											save model
											predict
											save card codes and their labels

feature engineering 	preprocessing -> standardize
											preprocessing -> create user cubes
											create convolutional autoencoder																												Needs to be tested
											train encoder																																						Needs to be tested
											encode samples																																					Needs to be tested
											visualize low dimensional data																													Needs to be tested
											save data																																								DONE

clustering 						k-means
											cluster analysis (silhouettes samples for cluster membership)


TODO:
normalize areas? keep as ordinal?
num of transfer as pie chart

Papers to get:
Reconstructing individual mobility from smart card transactions: a collaborative space alignment approach

Improvements to report:
transfer time focus on tail
thesis organization more technical, and methods per parts
Mention hierarchy in traffic areas will cause correlation
Attributes summary: numerical vs categorical
Section: deal with categorical -> tokenize, encode, bins
comment on ordinal structure vs one hot
random forests deal well with categorical (large node size), adaboost with missing info

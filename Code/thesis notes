preparation						cleaning																																							DONE
											information extraction (mode,line,stop,#trips,time bins)															DONE
											patching																																							DONE
											formatting																																						DONE

preprocessing					label data																																						DONE
											standardize																																						DONE
											create user cubes																																			DONE

feature selection 		load and sample dataframes	                                                          IN PROGRESS
											correlation tests (correlation, feature importance, chi2 and anova f value)						DONE
											select k best																																					DONE
											load user cubes																																				DONE
											flatten cubes into vectors
											save vectors

ensemble classifier 	create models (SVM, bayesian, random forest, mlp)
											train using K-fold with replacement
											ensemble (adaboost or bagging)
											test
											save model
											predict
											save card codes and their labels

feature engineering 	preprocessing -> standardize
											preprocessing -> create user cubes
											create convolutional autoencoder																												Needs to be tested
											train encoder																																						Needs to be tested
											encode samples																																					Needs to be tested
											visualize low dimensional data																													Needs to be tested
											save data																																								DONE

clustering 						k-means
											cluster analysis (silhouettes samples for cluster membership)


TODO:
tsne plots for these
num of transfer as pie chart
fix start/end hour 0. change to 24?
fix two trips one start -> merge trips?


NOTE:
currently transforming data over whole data set and not over train/test or cross validation.
 	pro: convenient because normalization is over type of feature, meaning 'TRAVEL TIME' over all trips BEFORE merging per user
				to change it would mean to grab feature slices from each cube, instead of values from each vector
				list of slices, instead of list of values (vector)
	con: scientifically correct? travel time and distance has large tail, thus the distribution really depends on the data

Papers to get:
Reconstructing individual mobility from smart card transactions: a collaborative space alignment approach

Improvements to report:
transfer time focus on tail
Mention hierarchy in traffic areas will cause correlation
Attributes summary: numerical vs categorical
Section: deal with categorical -> tokenize, encode, bins
comment on ordinal structure vs one hot
random forests deal well with categorical (large node size), adaboost with missing info
feature selection by some sort of ensemble, voting system

categorize mode, stop, line: zero cannot be used cause it means empty.

STEP BY STEP:
Preparation job
		 records number plot
		 vocabularies cummulative plot

Preprocessing job
    full: labeled, std -> dataframe, cubes
    full: unlabeled, std -> cubes

Feature selection code
     (maybe?) load all labeled dataframes, sample 10k and discard rest from memory
     Flatten -> vectors
Feature selection job -> vectors

Ensemble code
     three classifiers: SVM, trees, bayesian -> individual accuracy (save in txt file)
     adaboost ensemble -> accuracy (save in txt file)
     link sample to code -> two txt, one for commuters one for non-commuters
Ensemble job -> lists

Feature extraction code
     build autoencoder -> vectors
Feature extraction job -> vectors

Clustering code
    read vectors
    tune
    evaluate
    link sample
Clustering job -> lists

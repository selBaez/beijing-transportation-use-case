preparation						cleaning																																							DONE
											information extraction (mode,line,stop,#trips,time bins)															DONE
											patching																																							DONE
											formatting																																						DONE

preprocessing					label data																																						DONE
											standardize																																						DONE
											create user cubes																																			DONE

feature selection 		load dataframes							                                                          DONE
											correlation tests (correlation, feature importance, chi2 and anova f value)						DONE
											select k best																																					DONE
											load user cubes																																				DONE
											flatten cubes into vectors																														DONE
											visualize low dimensional data																												DONE
											save vectors																																					DONE

ensemble classifier 	create models (SVM, bayesian, random forest, mlp)																			Details to be finished
											train using K-fold with replacement 																									DONE
											ensemble (adaboost or bagging)																												TO DO
											test																																									TO DO
											save model																																						TO DO
											predict																																								TO DO
											save card codes and their labels																											TO DO

feature engineering 	Load cubes																																						DONE
											create convolutional autoencoder																											DONE
											train encoder																																					Needs to be tuned
											save model 																																						TO DO
											encode samples																																				DONE
											visualize low dimensional data																												DONE
											save data																																							DONE

clustering 						k-means																																								Details
											cluster analysis (silhouettes samples for cluster membership)													TO DO


TODO:
num of transfer as pie chart
fix start/end hour 0. change to 24?
fix two trips one start -> merge trips?


NOTE:
currently transforming data over whole data set and not over train/test or cross validation.
 	pro: convenient because normalization is over type of feature, meaning 'TRAVEL TIME' over all trips BEFORE merging per user
				to change it would mean to grab feature slices from each cube, instead of values from each vector
				list of slices, instead of list of values (vector)
	con: scientifically correct? travel time and distance has large tail, thus the distribution really depends on the data

Papers to get:
Reconstructing individual mobility from smart card transactions: a collaborative space alignment approach

Improvements to report:
transfer time focus on tail
Mention hierarchy in traffic areas will cause correlation
Attributes summary: numerical vs categorical
Section: deal with categorical -> tokenize, encode, bins
comment on ordinal structure vs one hot
random forests deal well with categorical (large node size), adaboost with missing info
feature selection by some sort of ensemble, voting system

categorize mode, stop, line: zero cannot be used cause it means empty.

STEP BY STEP:
Preparation job
		 records number plot
		 vocabularies cummulative plot

Preprocessing job
    full: unlabeled, std -> cubes

Ensemble code
     classifiers: bayesian -> individual accuracy (save in txt file)
     adaboost ensemble -> accuracy (save in txt file)
     link sample to code -> two txt, one for commuters one for non-commuters
Ensemble job -> lists

Feature extraction code
     build autoencoder -> vectors
Feature extraction job -> vectors

Clustering code
    tune
    evaluate
    link sample
Clustering job -> lists

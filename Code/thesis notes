preparation						cleaning																																							DONE
											information extraction (mode,line,stop,#trips,time bins)															DONE
											patching																																							DONE
											formatting																																						DONE

preprocessing					label data																																						DONE
											standardize																																						DONE
											create user cubes																																			DONE

feature selection 		load dataframes							                                                          DONE
											correlation tests (correlation, feature importance, chi2 and anova f value)						DONE
											select k best																																					DONE
											load user cubes																																				DONE
											flatten cubes into vectors																														DONE
											visualize low dimensional data																												DONE
											save vectors																																					DONE

ensemble classifier 	create models (SVM, bayesian, random forest, mlp)																			DONE
											train using K-fold with replacement 																									DONE
											ensemble (adaboost or bagging)																												Try soft voting
											evaluate																																							DONE
											save model																																						DONE
											predict																																								Predict on whole not implemented
											save card codes and their labels																											DONE

feature engineering 	Load cubes																																						DONE
											create convolutional autoencoder																											DONE
											train encoder																																					Only 150 iterations to avoid overfit
											save model 																																						DONE
											encode samples																																				DONE
											visualize low dimensional data																												DONE
											save data																																							DONE

clustering 						k-means																																								Run several times
											save card codes
											find their associated records
											cluster analysis (silhouettes samples for cluster membership)													TO DO


TODO:
num of transfer as pie chart
fix start/end hour 0. change to 24?
fix two trips one start -> merge trips?


NOTE:
currently transforming data over whole data set and not over train/test or cross validation.
 	pro: convenient because normalization is over type of feature, meaning 'TRAVEL TIME' over all trips BEFORE merging per user
				to change it would mean to grab feature slices from each cube, instead of values from each vector
				list of slices, instead of list of values (vector)
	con: scientifically correct? travel time and distance has large tail, thus the distribution really depends on the data

Improvements to report:
analyze results: ensemble two different classifiers would be better. try code for this
rewrite classification performance according to cluster results
remove side colorbar



STEP BY STEP:
Preprocessing job
    full: unlabeled, std -> cubes

Clustering code
    analysis

\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

\usepackage[final]{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor, graphicx, subcaption, float, enumitem, amsmath}

\newcommand{\selfnote}[1]{\footnote{\textcolor{red}{#1}}}
\newcommand{\domainDoubt}[1]{\footnote{\textcolor{teal}{#1}}}
\newcommand{\technicalDoubt}[1]{\footnote{\textcolor{blue}{#1}}}

\title{Pattern recognition of traveling behavior: Beijing use case}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Selene Baez  Santamaria \\
  \texttt{s.baezsantamaria@student.vu.nl}
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  Public metropolitan transportation is a data-rich domain that can benefit from data mining and machine learning techniques. Different stakeholders can benefit from insights regarding the traveling behavior of public transit users. However, traveling behavior consists of complex  spatiotemporal patterns that so far have not been analyzed as a whole.
  
  This project proposes a three dimensional representation for pattern recognition of traveling behavior. We carry out dimensionality reduction on this representation and compare supervised and unsupervised learning tasks for recognizing typical behaviors in users. First, ensemble models aid us on the task of binary classification of commuters (as labeled by self-reported survey data). Then, we characterize users by encoding their behaviors using an convolutional autoencoder, and performing clustering. The city of Beijing, China is posed as an use case for the project. 
  
  With 75\% average accuracy, our results on binary classification are not competitive with other studies on the field. However, labeled data obtained via surveys is typically noisy, and thus a high classification rate does not necessarily benefit Transportation specialists. In contrast, we found 7 distinct user groups on the clustering task leading to a better understanding of the users' routines and needs. In conclusion, we believe state of the art techniques for  unsupervised learning and local representations should be exploited in this domain. As such, the knowledge extracted can lead to better target policies and services in pro of improving the transportation network in large and complex cities.    
  
\end{abstract}

\newpage

\tableofcontents

\newpage
\section{Introduction}

\subsection{Urban public transportation}
% Why is public transportation important?
Urban public transportation includes systems that are available for use by anyone in urban areas. Its facilities are commonly composed by buses, subway/metro lines, light rails, tramways, trains, taxis and others. As a network, they provide service for the majority of citizens in urban areas.\citep{vuchic1900urban}

Figure \ref{fig:transportation/passenger} shows the passsenger transport usage, as million passengers per kilometre, in several different countries according to the Organisation for Economic Cooperation and Development (OECD). From all OECD countries, the United States, China, Germany, France, and Italy contitute the five countries with the most passenger transport, according to their reported data from 2015 or later.\cite{OECD2017passenger} 

\begin{figure}[H]
  	\centering
  	\includegraphics[width=\linewidth]{./images/OECD_passengers_absolute.png}
  	\caption{OECD countries and their passenger transportation data.}
  	\label{fig:transportation/passenger}
\end{figure}

Furthermore, historical data in Figure \ref{fig:transportation/passenger-trend} reveals the 15 years behavior for each of the aforementioned countries. Most of the countries show stability, with increase or decrease of less than 0.10 million passengers for European countries, and 0.5 million passengers for the United States. China, however, shows a trend with steep increase for most of the selected years. In fact, China's public transport usage doubled to 2.4 million passengers in 2015 from its less than 1.2 million passengers in 2000, 
  
\begin{figure}
  	\centering
  	\includegraphics[width=\linewidth]{./images/OECD_passengers_increase.png}
  	\caption{Historical data for the top six countries with most passenger transport usage. China (top-left image) has the steepest increase overall.}
  	\label{fig:transportation/passenger-trend}
\end{figure}

Though it is a more sustainable alternative compared to private car usage, public transport usage has a significant environmental impact, affecting noise and air pollution. Diesel buses, which make up a major part of public buses, have large fuel consumption needs and contribute significantly to CO$_{2}$ emissions. Even eco-friendly alternatives such as hybrid diesel buses are sensitive to operating conditions, as their fuel consumption may increase by up to 50\% when the on-board air conditioning is on.\cite{zhang2014real}

Public transportation directly relates to energetic demand as well, since its facilities are mostly petroleum or electricity based. Passenger transportation accounts for about 25\% of the global energy consumption. Furthermore, the transportation sector consupmtion increases at an annual average rate of 1.4.\% \cite{eia2016energy} This may bring further economical implications for countries with high public transportation demand.

\subsubsection{Who are the commuters?}
% Self proclaimed commuters
A major proportion of public transport users is represented by commuters, who are regular users of public transit with consistent spatiotemporal patterns in their travels. Driven by a routine, commuters travel back and forth from specific places, for example, from their home to work, school, or other similar locations. 

As commuters are frequent users of public transit, the  conditions of the public network directly influence their personal well being and generally impact their quality of life. Intuitively, if the commuting experience is unpleasant, daily travel can cause distress to commuters and/or even repel them from using the public transport at all. Several studies have looked into public transit evaluation from different perspectives, including commuters' needs \cite{mao2016commuting}. The most common aspects of it include: travel time, average speed, delays, accessibility, service coverage, crowded level, facilities quality, and fare rate. Weng et al \cite{weng2013bus} identified four indexes (Convenience, Rapidity, Reliability and Comfort) that summarize commuters priorities when choosing to travel by public transport.
 
From both of the above, the large presence of commuters and their known needs and preferences, it follows that identifying commuters and addressing their needs can help in creating a sustainable public transportation network. Public transit stakeholders should be able to understand the commuters' demands and its dynamics, consequently bringing long term planning and policies for improving the overall commuting experience.

\subsection{The city of Beijing}
% Why is beijing a good (challenging, rich, valuable, relevant) use case?
The city of Beijing, China presents a special case of urbanization and rapid industrialization. This is reflected in a sudden population growth of 20\% per decade since 1960, with the largest increase of 44\% in the last ten years. The latest official census in 2010 reported the urban agglomeration of Beijing (including Beijing itself and its adjacent suburban areas) having a population of 19,612,368 people. The UN World Urbanization Prospects estimates the 2017 population at over 22 million inhabitants. \cite{world2016beijing}

As a result of the population explosion, many environmental and social resources are under pressure. From the environmental side, one of the most notable issues is related to air pollution, due to the significantly high pollutant emissions in the city \cite{zhang2016air}. Similarly, the city's downstream river pollution is serious, with most regions of the Yellow river being unable to comply with the lowest water quality standards. \cite{wang2015studies} 

On the aspect of social resources, one of the main complications is mobility. In Beijing, public transport is the dominant mode of transportation, accounted for 44.0\% of all trips compared to 32.6\% attributed to private cars \cite{mao2016commuting}. In 2008, the total ridership was 6.5 billion travels. Though the network is continually expanding, it is a fact that public transport is overcrowded, constantly reaching over 100\% capacity \cite{beijing2009research}.

Beijing public transport is composed of buses, subway and bicycles. The three types can be accessed by using a single smart card. 

\begin{description}
\item[Bus:] In 2015, there were 876 bus lines with 23,287 buses in operation. The bus network is the most extensive mode of transportation, expanding over 20,186 km. It observes an average daily traffic volume of 10.98 million passengers, with the highest daily volume reaching 13.07 million on one day. \cite{beijing2016annual}

\item[Subway:] The Beijing subway has 18 lines with 334 stations, of which 53 are transfer stations. In 2015 it had an operating length of 554 km, with 5,024 vehicles running. \cite{beijing2016annual} Its network is split by two operators: the state-owned Beijing Mass Transit Railway Operation Corp (operating 15 lines), and the joint Hong Kong venture Beijing MTR Corp (operating 3 lines).

Beijing's subway has an average daily traffic volume of 9.11 million passengers, with a maximum recorded volume of 11.66 million passengers. As such, it is the second busiest metro system in the world, providing 3,410 million annual journeys. Compared to the service provided in 2012, the system observed a 39\% increase in usage by 2014. It is also the second longest metro network, surpassed by Shanghai by only 21 km.  \cite{uitp2015world} 

\item[Bicycles:] Beijing first implemented public bicycle systems in 2012. As of 2015, in total, 67,000 bikes are available for rental with 2,700 pick up/drop off points spread across the city. \cite{beijing2016annual}
\end{description}


\subsection{Smart cards and Big Data}
Smart cards present us with a straightforward way of massively collecting daily data. In the last years, smart card systems have become more popular in the Transportation domain, making it possible to monitor travelers transactions and facilitating fare collection. Several cities have implemented such systems, for example the Octopus card in Hong Kong\cite{chau2003octopus}, compass card in Vancouver, Oyster card in London \cite{blythe2004improving}, OV-chipkaart in The Netherlands \cite{de2008analysis}, and Yikatong card in Beijing \cite{chan2010tactical}, to name a few.

In Beijing, over 90\% of public transit users are smart card holders. There is a significant incentive for using the Yikatong smart card since bus rides are heavily subsidized (users have only to pay 50\% of the full price)\cite{ma2017understanding}. Moreover, the Yikatong smart card system is also integrated with taxi, electricity and sewage payments, making it convenient to use as a general paying method.

\paragraph{Data quantity}

Placed in context, public transit systems serve at least hundreds of users daily, where a typical user performs several trips a day, every day. On the specific case of Beijing, there are hundreds of thousands of smart cards gathering between 5 and 16 million records (trips) a day, among a large complex network containing thousands of routes and tens of thousands of stops. 

\paragraph{Data quality}

However, though smart cards exponentially increase the quantity of data, they do not completely guarantee its quality. As is, some aspects of the trips cannot always be faithfully recorded but are inferred (for example, the transfers within the subway system when no check-in/out is done at changing trains). Furthermore, some fields are sometimes simply missing or incorrectly recorded due to malfunctions and situations out of control. 


Given the large amounts of data collected and its nature, the analysis of such becomes challenging. Transit smart cards are capable of recording spatiotemporal information at an individual level over long periods of time. This generates a large volume of historical data that only tailored big data techniques can deal with. 

\subsection{Project motivation}
This project performs an interdisciplinary study between the areas of Artificial Intelligence and Metropolitan Transportation. It is focused on introducing data mining techniques to a data rich domain. 

% Each science benefits
The area of Artificial Intelligence is able to provide dozens of prediction algorithms. Though constantly under refinement, it is time for state-of-the-art techniques to be applied, tested and validated under real and large impact situations to test their ability to deal with noisy streams. Comparably, given the ever growing complexity of urban mobility, domain experts must focus on analyzing trends and insights instead of curating and making sense out of raw data. As such, introducing these state-of-the-art techniques into the Metropolitan Transportation domain can aid to unravel massive human behaviors and reveal patterns and trends in mobility.

\subsubsection{Societal context}
% description, prediction and preescription
Identifying and analyzing mobility patterns may have different goals, from description, prediction or prescription, all of which affect their stakeholders directly. 

A descriptive analysis determines how people use the public transit. It can pinpoint chaotic hotspots in the city, peak hours, popular routes or other behaviors. A predictive analysis investigates how will people use the transit in the future or under new circumstances. For example, public transport usage projections in the years to come directly affects environmental models trying to improve air and water quality, energetic demand or other natural  and economic resources.

Finally, a prescriptive analysis focuses on how should the different stakeholders deal with mobility behaviors. For example, the government as well as transport management and operators would gain invaluable spatial and temporal insights regarding commuters' behaviors. This insight may lead to tangible results, including policies for increasing the efficiency of the public transit network, adjustable travel fares tailored to the most relevant mobility patterns, incentives to relieve peak hours and thus traffic congestion, urban planning for residential and industrial land use, and others.

Given that Beijing has a widely spread data collection system, combined with formidable institutions capable of introducing new measures in their public transportation network, this city is an excellent use case where the results of an in-depth study can generate actionable plans and bring benefits in the short and long term.

Furthermore, the social context of Beijing presents specific opportunities for improvements where the full power of data analysis and its impact can be tested. For example, the city of Beijing faces a large imbalance between residential and working areas. Due to urban expansion, most residents have been forced to move to suburban areas due to the lack of affordable housing, regardless of having their work environments within the six Ring Roads \cite{zhou2014commuting}. Investigating and targeting this group could alleviate the pitfalls of long distance commuting. 


\subsubsection{Scientific context}
% Improve individual and collective analysis
Mobility patterns in metropolitan areas follow complex swarm behaviors. Based on individual travels and routines, travelers exhibit distinguishable characteristics on a larger scale. Both individual and collective levels of understanding are crucial for Transportation experts. In order to explore both levels, Metropolitan Transportation studies typically focus of the usage of surveys. These surveys are targeted to reach travelers on an individual level, while large scale indicators and aggregated data are taken to investigate their collective behavior. 

These methods have several disadvantages. On the one hand, surveys are costly to implement, and in general have problems related to small non-representative samples. Even when these problems are escaped, the usual quality versus quantity trade off is present, reducing the confidence of the collected information. On the other hand, large scale measurements (i.e. total passenger flow) miss the interactions between individuals that cause the collective behavior.

On top of this, an important consideration on the Metropolitan Transportation domain is that the data collected by smart cards is unlabeled. This means that traveling behaviors are not assigned to known specific categories, making it hard to validate and evaluate. Typically, this issue is address by asking some sample users -via surveys- how they categorize themselves (for example, if they consider themselves to be commuters) and then extrapolating this profile to new users. However, self-reported data by itself has bias problems, therefore introducing noise or false patterns. 

Fortunately, the field of pattern recognition has seen major development in the last years. Nowadays, there exist machine learning and other data mining methods specialized in analyzing disaggregated complex information. Data analysis can be as general is specialized as needed, producing reliable and comprehensible information and visualizations. Furthermore, unsupervised tools have arisen that find patterns based on the data alone, thus being independent from the aforementioned biases.


\subsection{Thesis organization}
The thesis is organized as follows: On the next section we perform a \textit{Literature review} to explore previous work on mining smart card transit data. We also summarize current representation and pattern recognition methodologies for dealing with complex spatiotemporal data.  

Subsequently, we establish the \textit{Research framework} where we explicitly state the objectives and research questions of this project. As a result, we limit the project's scope and clearly define the most important terms to be used. 

We continue to describe the \textit{Methodology} thoroughly. This consists of an extensive description of the data and its characteristics, our proposed 3 dimensional representation for spatiotemporal data, and the data mining approach to follow, including supervised and unsupervised learning techniques for dimensionality reduction and pattern recognition.

Following this, we identify three distinct stages of the project: \textit{Data preparation and preprocessing}, \textit{Commuters identification}, and \textit{Traveling behavior clustering}. In the \textit{Data preparation and preprocessing} section we describe the pipeline for processing raw data, extracting trip attributes and finally creating the proposed 3D representation of an user's traveling behavior. 

The section on \textit{Commuters identification} describes a supervised learning approach for classifying labeled data, using feature selection and ensemble models. Its counterpart, the section on \textit{Traveling behavior clustering} describes an unsupervised  learning approach to recognize similar traveling behaviors, using feature extraction (by means of an autoencoder) and clustering algorithms. 

Finally, we gather conclusions regarding the proposed representation. We compare both supervised and unsupervised approaches and explore future work opportunities. 

\newpage
\section{Literature review}
In this section we look at studies within the last decade that are related to smart card transit data. First, we summarize the approach and the most relevant findings of each paper in order grasp a broad view of the Transportation domain. Secondly, we explore representations for spatiotemporal data and compare the way traveling behavior is usually represented in the Transportation domain, and other types of representations available in the Artificial Intelligence domain. Thirdly, we discuss techniques for pattern recognition through supervised and unsupervised learning. %Finally we explore some pioneer work in end-to-end learning. 

\subsection{Data mining on transit card data}
% What questions and approaches are common. How do they interpret traveling behavior? What's the volume of data?
With the introduction of smart card systems in large cities, several studies have aimed to extract knowledge from the large amounts of data collected. Many of this studies focus on analyzing traveling behavior, which is regarded as a spatiotemporal mobility pattern. Though different in their methodology, results concerning commuters are duplicated across studies. As the spatial and temporal regularity of commuters' travel behavior is evident in their smart card data, they pose an excellent opportunity of study.

Morency et al. \cite{morency2007measuring} study spatio-temporal variability in Canadian smart card data. On the one hand, they examine spatial variability by measuring the number of distinct stops a smart card user visits, and the frequency of each stop. On the other hand, they examine temporal variability by clustering the boarding times of each type of smart card. Using these features, they observe the week to week variability for each of the five types of transit card available (Adult-interzone, Adult-express, Adult-regular, Elderly and Student). Their findings show that commuter types of cards visit a smaller range of bus stops compared to non-commuter types. Therefore, a small number of stops account for a high proportion of commuter's boardings. Additionally, commuters have the highest proportion of zero-boarding days on weekends.

Bhaskar et al. \cite{bhaskar2015passenger} are concerned with passenger segmentation using Australian smart card data. First, they perform a two level DBSCAN algorithm for investigating spatial patterns, where the first level clusters Destination stops and the second level clusters Origin stops.  From this they extract frequent Origin-Destination (O-D) pairs. Separately, they applied DBSCAN to temporal features to determine most frequent boarding times.  As such, they characterize each user by the percentage of journeys they perform between the regular O-D, and the percentage of journeys they perform during their habitual times. Users with at least 50\% spatial and temporal regularity are thus classified as transit commuters; while users with no evident spatial or temporal pattern are classified as irregular passengers. The authors find that while most (64\%) of the passengers riding the public transit are irregular passengers, it is transit commuters who bring the most (46\%) revenue. Furthermore, they find that irregular passengers prefer high frequency routes significantly more than transit commuters, arguing that commuters are usually on a time habit, and thus are more willing to check and adapt to public transit timetables.

Tu et al. \cite{tu2016impact} follow a supervised learning approach to classify public transit users in Beijing as commuters or non-commuters. In order to produce labeled data, they convey an online survey asking for travel patterns and smart card ID. Matching the ID to the journeys recorded by smart card during the span of one week, they collect records associated to 978 travelers. The classification is then performed by a Support Vector Machine (SVM), which reaches up to 94.24\% accuracy.

Langlois et al. \cite{langlois2016inferring} present an innovative representation for smart card data. Using four weeks worth of data from London Oyster cards, they represent the card information as a time-ordered sequence of inferred activities.  11 clusters are found and characterized by evaluating socio-demographic variables like age, employment, annual household income, children per household and vehicles per household. The authors further grouped the clusters under "working day", "home bound", "complex activity pattern" and "interrupted pattern" categories. Their findings show that four clusters, grouped under the "working day" category have significantly different activities during weekdays as compared to weekends, with some avoiding transit during the weekends and others visiting different areas.  Four more clusters, grouped under the "home bound" category, are characterized by staying mostly at their primary area and low number of traveled days. 

One of the latest work on the field corresponds to Ma et al. \cite{ma2017understanding}. The objective of their work is to determine a scoring function for travelers that can correctly identify them as commuters, or non-commuters. In their work, they cluster stops using an improved DBSCAN algorithm. They engineer features for representing the frequency in which travelers follow spatio-temporal patterns. Travelers are then clustered according to these features following the ISODATA algorithm. As an output of the clustering, optimal cutoff levels in the scoring function were determined. As a result, evaluating a traveler does not depend on clustering centroids, but only on calculating the commuting score. This, as expressed by the authors, reduces computing time and treats each traveler independently from the others, which is not true for clustering algorithms.

A common practice, as used by \cite{ma2017understanding}, \cite{langlois2016inferring}, and \cite{morency2007measuring} is to divide the day into -hourly or half-and-hour- time bins. Bhaskar et al.  \cite{bhaskar2015passenger} recognize this as a problem in the field, by pointing out that this design choice segregates journeys from 9:59 AM and 10:01 AM even though they intuitively belong to the same behavior. 

\subsubsection{Volume of data}
The volume of data collected by smart card systems is massive and is usually impossible to analyze all of it at once. The volume of the samples analyzed by previous work ranges from hundreds of smart cards to tens of millions of smart cards, leading to up to hundreds of millions of individual smart card transactions. The details of the revised literature are summarized in Table \ref{table:volumeData}.
 
\begin{table}[H]
\centering
\begin{tabular}{||c c c c c||} 
 \hline
 Authors & Year of publication & Records & Unique smart cards & Time span \\ [0.5ex] 
 \hline\hline
 Morency et al. \cite{morency2007measuring} & 2007 & 2.2 million & 7,118 & 277 days \\
 Ma et al. \cite{ma2013mining} & 2013 & Unknown & 3 million & one week \\
 Ortega \cite{ortega2013classification} & 2013 & 65 million & 5.7 million & one week \\
 Bhaskar et al. \cite{bhaskar2015passenger} & 2015 & 34.8 million & 1 million & 4 months \\ %, working days only
 Tu et al. \cite{tu2016impact} & 2016 & 8,067 & 978 & one week \\ 
 Langlois et al. \cite{langlois2016inferring} & 2016 & 3 million & 33,026 & four weeks \\
 Ma et al. \cite{ma2017understanding} & 2017 & 364 million & 18 million & one month\\ [1ex] 
 \hline
\end{tabular}
\caption{Volume of data analyzed by different authors}
\label{table:volumeData}
\end{table}

Given the limit on how many records can be examined per study, researchers usually face the decision to reduce the dataset to a manageable size. As such, there exists a trade off between the number of unique smart cards and the time span of the collected data. Some researchers, like Ortega \cite{ortega2013classification}, decide to analyze a large population over short periods of time. Others, like  Bhaskar et al. \cite{bhaskar2015passenger} choose to explore long term behavior thus having to reduce the population size. 

However, it is worth noting that the total number of records studied has increased overtime. This most likely is due to the trends of increased computational power and the design of optimized mining algorithms. A clear example is the study by Ma et al. \cite{ma2017understanding} published just this year that was able to include data of a significantly large population over a month. 

\subsection{Representing spatiotemporal data}
\label{sec:representations}
As Marr puts it, representations make explicit different types of information implicit in entities \cite{marr1982computational}. Thus, representations mainly differ in the information they describe and the way they describe it. Usually, representations are generated to achieve a information processing goal. Thus, the value of a representation depends on the purpose of the task it will be used for. 

Data representation is one of the fundamentals in data mining. Ideally, the representation of a data point is comprehensive of its underlying unique factors and leaves out unnecessary or noisy information. Furthermore, the format for the representation must be akin to the types of information that data mining algorithms can process. Therefore, finding suitable representations for complex concepts like space and time is not an easy task.

\subsubsection{Traditional feature engineering}
Human mobility is intrinsically tied to spatio-temporal properties. Still, the greatest amount of studies analyze public transit journeys by separating spatial features from temporal features. Furthermore, in general scalar aggregated features are used for users characterization. Some examples are:

\begin{itemize}
\item \textbf{Frequency indicators:} number of traveled days \cite{bhaskar2015passenger} \cite{langlois2016inferring} \cite{ma2017understanding}, number of journeys \cite{bhaskar2015passenger}, number of times a stop was visited \cite{morency2007measuring}, number of days with zero boardings \cite{morency2007measuring}, most frequent home/work stop \cite{ma2017understanding}, most frequent home/work route \cite{ma2017understanding}, most frequent departure time from home/work \cite{ma2017understanding}, number of trips to the most frequent home/work stop\cite{ma2017understanding}, number of trips following the most frequent home/work route \cite{ma2017understanding}, number of trips during most frequent departure time from home/work \cite{ma2017understanding}

\item \textbf{Range/coverage indicators:} distinct stops visited \cite{morency2007measuring}, spread of days between the first and last journey \cite{langlois2016inferring}

\item \textbf{Calendar-based indicators:} observed day \cite{morency2007measuring}, day of week \cite{morency2007measuring}

\end{itemize}

Though popular among the Transportation domain, hand engineered features may present great disadvantages. While these features are intuitive and semantically meaningful for Transportation specialists, they do not always represent distinctive properties of users or their public transit journeys. Therefore, the time invested in designing and producing features may not always payback in relevant findings. 

This case can be compared with the trends seen in Computer Vision. A few decades ago, most approaches for Image Understanding were focusing on designing features to describe them (i.e. SIFT). However, after the rapid development of Neural Networks in the last years, the most successful Vision applications are based on learned features. As Nithin and Sivakumar explain, hand crafted features are time consuming, fragile and incomplete, thus being outperformed by automatically extracted features which learn better the underlying representations in images \cite{nithin2015generic}. 


\subsubsection{Feature extraction}
Feature extraction refers to the creation of features that represent the underlying characteristics of data. These features are automatically created, solely from the data, in either statistical or learned ways. One popular way for extracting features is by using methods for dimensionality reduction, which beyond finding representations further tackles the curse of dimensionality. 

\textbf{Principal Component Analysis}

One of the most robust algorithms for this is Principal Component Analysis (PCA) which is a mathematical tool used across several domains. By doing matrix manipulation, PCA extracts eigenvalues and eigenvectors from a given dataset. The top eigenvectors represent the ways in which the data points are more different from each other.

An isolated work related to this was performed by Langlois et al. Following a unique methodology for engineering features, first they represent the travel data per user using a three dimensional matrix where $x$ represents the day in the four week period, $y$ represents the hourly time bin, and $z$ represents the area where the inferred activity took place, encoded as a one hot vector. The authors perform PCA for dimensionality reduction, based on Eagle and Pentland's eigenbehaviours \cite{eagle2009eigenbehaviors}. An analysis of the average correlation of the first 13 components, results in the selection of the first 8 components as the most informative and stable. The projections of a user sequence onto these components (called weights) constitute the features to be clustered using k-means. \cite{langlois2016inferring}

\textbf{Autoencoders}

Following the same main principle as the first neural networks, autoencoders are highly connected networks that map high dimensional data to low dimensional spaces. Primarily used for images, the goal of an autoencoder is to deconstruct an input image onto a representation, and reconstructing the image again, with a minimum loss of information. Each of these are called the Encoder and Decoder modules, respectively. Together these are learning modules that tune its parameters until achieving sufficient performance. 

Much research has been done on autoencoders, leading to several variations of them. Denoising autoencoders result in a more robust algorithm, since they get a corrupted image as input, but aim for reconstructing the original image. Therefore, the autoencoder does not simple map one instance to a representation, but truly learns the significant characteristics present in the data \cite{nithin2015generic}.  

To the best of our knowledge, these techniques have not been introduced to the Transportation domain. 


\subsection{Pattern recognition on spatiotemporal data}
% Supervised and unsupervised learning

\subsubsection{Classifying algorithms}
The domain of Metropolitan transportation faces a specific problem: although smart card systems have allowed massive collection of data, this data is not labeled regarding commuting behaviors. Additionally, obtaining labels for smart card data is expensive and unreliable, since it has to be acquired through surveys or interviews. Furthermore, even when labels are obtained, the amount of labels obtained is often insufficient for big data analysis. It is due to these reasons, that most studies are inclined to used unsupervised learning techniques. 

One of the few studies that uses labeled data corresponds to Tu et al. They obtain 978 labeled records, with an almost equal distribution of records over both classes (49.18\% related to commuter samples and 51.82\% related to non-commuter samples). They solve the issue of limited samples by selecting a model that is not heavily affected by sample size: Support Vector Machines. Using a linear kernel, and 7:3 ratio for train-to-test sets division, their results report a 94.24\% accuracy over a test set of 295 samples. 

\subsubsection{Clustering algorithms} 
If labeled data is not available, then unsupervised learning techniques must be applied. There is a large variety of clustering algorithms available nowadays, however not all of them are suitable for all types of data and purposes.

\textbf{Hierarchical clustering}

Langlois et al. \cite{langlois2016inferring} use agglomerative hierarchical clustering for areas clustering. In order to infer the user-specific activities, all stops or stations visited by each user are clustered by merging the two closest areas until a threshold distance is reached. Their algorithm also considers the distance between stops and the frequency of travel between them. Therefore, different activities are likely to be associated with different areas.

\textbf{Partitional clustering}

The K-means algorithm is the most widely used method for partitional clustering. It requires having a predefined number of clusters to fit the data to. 

Morency et al. \cite{morency2007measuring} use K-means for clustering hourly boarding times according to card type. They apply Hamming distance (representing the percentage of data between two elements) and a combination of batch and online updates. Through empirical tuning, they select to find four clusters per card type. It is worth noting that by using a card-day unit, they allow a card to belong to a different cluster according to the day of travel. As every card type is composed of four boarding patterns, travelers are not restricted to follow a routine everyday, but can exhibit different behaviors on different days. For example, the Adult-regular card type contains a 9:00AM-and-5:00PM-boarding cluster and a no-boarding cluster. Thus, a user of this card could belong to the first cluster on weekdays and to the second cluster on weekends. 

Bhaskar et al. \cite{bhaskar2015passenger} apply K-means for binary classification purposes. As such, they classify frequent and infrequent transit users, using the number of traveled days and the number of journeys made as features. Unfortunately, K-means performs poorly since no distinct clusters are evident. The most likely cause for the previous is the strong correlation between traveled days and journeys, combined with the authors oversight of whitening and standardization techniques. 

Langlois et al. \cite{langlois2016inferring} use K-means to find clusters of activity sequences. They employ specialized sampling techniques, like bootstrapping, to deal with big data.  Moreover, they tune the algorithm parameters using the DB-index, which is the ratio of the within cluster distances to the across cluster distances. They find two optimal number of clusters (4 and 11), out of which they select the largest to provide the most detailed segmentation. They further perfection the algorithm by using k-means++initialization over 150 replications. Additionally, this paper acknowledges that clustering techniques are sampled based, which means different samples may find different optimal solutions. The authors validate their approach by analyzing the stability of the clusters over samples obtained at different points in time. By extracting the same number of clusters and fitting the samples to each set, they find that 91\% of users are assigned to their equivalent clusters. 

\textbf{Density based clustering}

Density based algorithms excel at dealing with anomalies, since they ignore low density areas and interpret them as noise. They do not required a redefined number of clusters and adapt to find clusters of any size. The required parameters for DBSCAN are a maximum reach distance $\epsilon$ and the minimum number of points per cluster.

Bhaskar et al. use three DBSCAN algorithms to cluster Origin stops, Destination stops, and boarding times. For each of the previous, they tune the algorithm parameters by fixing a domain reasonable $\epsilon$ (1000 m walking distance or 5 min variance in boarding time), and selecting the minimum points by comparing the percentage of data considered to belong to any cluster as opposed to data considered to be noise given the par-specific parameters \cite{bhaskar2015passenger}. 

Ma et al. use an improved DBSCAN algorithm to cluster bus/subway stops. In their approach, abnormal stops are not considered noise, but are allowed to be re-clustered by splitting large clusters into several smaller clusters. 

Though clustering algorithms are common in the field, they are not always used for classifying users. For example, Bhaskar et al. use density based clustering for engineering regularity features. However, the classification of users is rule-based according to which feature (spatial or temporal regularity) is stronger in each user \cite{bhaskar2015passenger}. Morency et al. use partitioning clustering to characterize existing user categories according to their boarding times \cite{morency2007measuring}. 

As a conclusion, we note that while there has been research applying basic clustering and classification algorithms, most studies lack further specialized data mining techniques for preprocessing data, tuning algorithms parameters, and/or visualizing results. 


\newpage
\section{Research framework}
The underlying goal of this project is to find an accurate spatiotemporal representation for public traveling behavior while accounting for big data constraints and the inherent data nature. The main two objectives are:

\begin{description}
\item[Objective 1] To identify commuters (as labeled by self reported survey data) based on their routine patterns. \label{eqn:obj1}
\item[Objective 2] To group users with similar travelling behaviors. \label{eqn:obj2}
\end{description}

Combined, these objectives characterize public transit users in the city of Beijing. For this project we decide to use one month worth of smart card data, since we believe it to be a long enough period to see different traveling patterns, while keeping the data at a manageable level. 

\subsection{Research questions}
The main objectives is further broken down into answering the following research questions: 

\begin{enumerate}

\item How can spatiotemporal features be analyzed as a unit?

\item What are the most relevant features when identifying commuters?

\item How accurately can commuters and non-commuters be identified using an ensemble model? 

\item How many distinct behaviors are present among public transport users in Beijing?

\item How does feature selection and feature extraction compare to each other in the transportation domain?

\end{enumerate}

\subsubsection{Definition of terms}
A commuter is a public transit user whose smart card data reveals repeatable patterns in time and space. Though commuters are usually associated with Monday to Friday 9:00am to 5:00pm schedules and a single job/residence pair, in this work we extend the definition to any routine travel pattern. This flexibility allows us to include travelers with stable yet rare commuting schedules, such as night workers, weekend workers and evening workers. Furthermore, we can also include travelers with more complex spatial patterns, for example people with more than one working location or residence.  

A trip is a sequence of smart card transactions, including transfers, performed by the same user to travel from an origin to a destination. A trip is also represented as a record in the data, as it will be further explained in Section \ref{sec:data}

A transfer is a change in transportation mode, or a change in vehicles whenever a smart card has to be checked within the same transportation mode. Transportation modes include Bus, Subway, and Bike. 

We make the assumption that smart card IDs and users have a one to one relationship, meaning each user has exactly one card and each card is used by exactly one user. As discussed with domain expert Quian Tu, although some people may own more than one card, this is a minority. Thus, the assumption holds for the majority of travelers. 

\subsection{Scope and structure}
This project is divided three main stages: 

\begin{description}[align=left,labelwidth=2cm]
\item[PART I: Prepare and preprocess the data using Big Data techniques] In this part we focus on research question 1. Techniques for cleaning, knowledge extraction, categorization, patching and standardization are used and tailored to the data. From this, we build an appropriate 3 dimensional representation for each user's traveling behavior. This part corresponds to Section \ref{sec:partI}.


\item[PART II: Classify commuters versus non-commuters by using an ensemble model]  In this part we focus on research questions 2 and 3. First, we perform feature selection in order to identify the most informative features and disregard redundant information. An extensive analysis of spatiotemporal properties is done, combining transportation domain knowledge, machine learning and statistical tools. Subsequently, we create a classifier using ensemble models and discuss its performance. This part corresponds to Section \ref{sec:partII}


\item[PART III: Users clustering according to patterns in their travel behaviors.]  In this part we focus on research question 4. First, we do feature extraction with the goal of reducing the dimensionality of the data. This is done via a convolutional autoencoder. Finally, we cluster the low dimensional representation using k-means clustering and do cluster analysis to understand the underlying pattern of each cluster. This part corresponds to Section \ref{sec:partIII}
\end{description}

Figure \ref{fig:flowchart} displays a flowchart for the stages and their connection.

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{./images/flowchart.png}
  \caption{Project flow}
  \label{fig:flowchart}
\end{figure}

\newpage
\section{Methodology}
\subsection{The data}
\label{sec:data}
The data used in this project is provided by government agency of Beijing Transportation Operations Coordination Center (TOCC), facilitated by the College of Metropolitan Transportation at Beijing University of Technology. Every record in the data represents a trip performed by a specific smart card. As such, it contains the following data fields:

\begin{itemize}
\item Data date: Year, month and day that the trip was made
\item Card code: Card identification number
\item Path link: Mode of transportation. B stands for bus, R for subway, Y for bicycle. Transfers between modes are shown by a dash. \footnote{Example: B-B represents a Bus to Bus transfer.} 
\item Travel time: Time spent in vehicles, measured in milliseconds
\item Travel distance: Distance traveled, measured in meters as performed by route. 
\item Transfer number: Number of changes in travel mode during the trip. 
\item Transfer total time: Total time spent in transfer, measured in milliseconds\item Transfer average time: Time spent in transfer, divided by number of transfers. Measured in milliseconds
\item Start/End time: Time stamp of when the trip started/ended. Date and time up to milliseconds precision
%Traffic zones
\item On/Off small traffic area: Integer ranging from 1 to 1911
\item On/Off middle traffic area: Integer ranging from 1 to 389
\item On/Off big traffic area: Integer ranging from 1 to 60
%Municipal zones
\item On/Off ring road: Integer ranging from 1 to 6
\item On/Off area: Integer ranging from 1 to 18
\item ID: record identification number created by joining the following: hour of the beginning of the trip | time stamp of beginning of the trip | card code performing the trip
\item Transfer detail: Mode of transportation, as well as line/route number and stations for boarding and alighting. More detail provided in Section \ref{sec:tripParsing}
\end{itemize}

Full privacy of card users is ensured, as there is no personal data linking card codes to specific individuals. 

The traffic zones (small, middle and big areas) are administrative divisions by the Beijing Municipal Institute of City Planning and Design (BICP). They are specific in different degrees, as shown in Figure \ref{fig:data/traffic_zones}. In general, the division principles correspond to the geopolitical environment and administrative planning, for example roads, villages and others. The 6 ring road and 18 areas districts are administrative divisions by the Beijing Municipal Government. The division is unique in Beijing. The 18 districts and counties are shown in Figure \ref{fig:data/18areas}. According to domain expert PhD. Liang Quan, these divisions are sufficiently informative for traffic analysis \cite{liang}.

\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{.3\textwidth}
  	\centering
  	\includegraphics[width=\linewidth]{./images/big_traffic_zone.jpg}
  	\caption{Big zones}
  \end{subfigure}
  \begin{subfigure}[b]{.3\textwidth}
  	\centering
  	\includegraphics[width=\linewidth]{./images/middle_traffic_zone.jpg}
  	\caption{Middle zones}
  \end{subfigure}
  \begin{subfigure}[b]{.3\textwidth}
  	\centering
  	\includegraphics[width=\linewidth]{./images/small_traffic_zone.jpg}
  	\caption{Small zones}
  \end{subfigure}
  \caption{Traffic zone division}
  	\label{fig:data/traffic_zones}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=.8\linewidth]{./images/beijing_18areas.png}
  \caption{Beijing's Districts and its Counties}
  \label{fig:data/18areas}
\end{figure}

Every day, more than 13 million trips are performed, with approximately 5 million corresponding to subway trips, 8 million corresponding to bus trips, and 100,000 corresponding to bicycle trips. All records corresponding to one day are saved in a single csv file. 

In this project we examine one month worth of data, corresponding to November 2015. The month of November is chosen because it does not overlap with holidays and has a relatively stable weather, hence diminishing the variance between bicycle and bus/subway traveler preferences. this, we hypothesize, will maximize the data quality. As a result, the data to be analyzed is divided into 30 csv files, requiring a total storage space of 53GB.

\subsubsection{Special considerations}
The previous description corresponds to the data as delivered by the TOCC. As such, it is the result from processing the raw records at the collecting phase. Some special considerations concerning this processing are explained below:

\begin{description}%[align=right,labelwidth=2cm]
\item[Travel distance by bike:] Since bicycles do not have predefined routes, the distance cannot be directly recorded. However, it is inferred by using the travel time and a static average speed for cyclists. 

\item[Subway transfer:] Transfers between subways lines of the same operator cannot be tracked since a single check-in gives access to the traveler to all the subway network. In order to infer the transfer detail, the A* algorithm is used to calculate the most likely transfer sequence, given the boarding and alighting stations. 
Furthermore, similarly to the bicycle missing information, the transfer time inside the subway system cannot be directly recorded. Using a static average walking speed and the known distance in transfer stations, the transfer time is calculated. 

\item[Transfer information:] The path link and transfer number fields are extracted from the transfer detail field. Similarly, the transfer average time is calculated from the transfer total time and transfer number fields. 
\end{description}


\subsubsection{Labeled and unlabeled data}
\label{sec:datasets}

\begin{figure}[h]
  \centering
  \includegraphics[width=.8\linewidth]{"./images/unlabeled data"} %TODO update image
  \caption{Relation between labeled and unlabeled data.}
  \label{fig:method/unlabeled}
\end{figure}


\begin{description}%[align=right,labelwidth=2cm]
\item[Commuter classification:]
Classification is a supervised learning task, where every training data sample requires an associated label determining its true class. In case of commuter classification, this translates to having smart card codes associated with either a "commuter" or "non-commuter" label. Such data is expensive to obtain and limited in principle, since it obtained by asking public transit users directly if they are commuters or not. Thus, in general, annotated data is not available, and labeling new records falls beyond the scope of this project. 

As a solution for the above, we take advantage of the dataset used by Tu\cite{tu2016impact}. This dataset corresponds to trip records performed during a week in January 2015, and it contains labels for 978 smart cards, collected and validated via surveys. The original dataset distribution is composed by:

\begin{itemize}
\item 6439 records of 481 commuters
\item 1628 records of 497 non-commuters
\end{itemize}

As mentioned in Section \ref{sec:data}, this project considers data corresponding to November 2015. Since both datasets correspond to the same year, we believe the labels to be reasonably relevant for both datasets. As such, in order to construct an extended labeled dataset, we take the 978 labeled smart card codes from Tu's dataset, and search for their corresponding records in our dataset. As shown in Figure \ref{fig:method/unlabeled}, a great disadvatage of this is that it tremendously reduces the size of the dataset to less than 1\% of its original size. 

The reduced labeled dataset is used for Part II (Section \ref{sec:partII}) of this project. 

\item[Behavior clustering:] 
Clustering, being an unsupervised learning task, does not require labeled data. Therefore, for this task we attempted to use the full dataset. Though rich in its contents, this posed some computational challenges that fall beyond the scope of this project. 

Therefore, we build a smaller dataset by sampling 100,000 user card codes. These codes are non-overlapping with the labeled card codes. This reduced unlabeled dataset is used for Part III (Section \ref{sec:partIII}) of this project.
\end{description}

\subsection{Spatio-temporal representation}
\label{sec:structure}
% 3D locality, channels, bins
One of the main contribution of this project is the novel representation of public transit travel behavior. We consider that the travel behavior of a public transit user is composed by a) the distribution of trips over time, and b) the spatial, temporal and general attributes of each of the trips. 

On the one hand, the distribution of trips over time is a complex continuous distribution. However, according to the Transportation domain standard, it can be approximated in a discrete manner. For this purpose, we divide each day into one hour bins. Then, we consider that each bin can be "occupied" by a trip, or empty if there was no traveling at that moment. 

On the other hand, each trip possesses different attributes of different nature. Fields like \textit{Travel time} and \textit{Travel distance}, for example, have continuous positive values within a reasonable range specific to the city of Beijing. In contrast, fields like \textit{Traffic areas} and \textit{Ring road}, for example, contain categorical values. Therefore, the information from smart card transactions is processed to extract relevant the attributes. 

We propose a 3 dimensional data structure to contain the monthly travel information of a public transit user. The conceptualization is pictured in Figure \ref{fig:data_mining/3D_structure}.

\begin{figure}[H]
  \centering
  \includegraphics[width=.9\linewidth]{./images/3D_structure.png}
  \caption{Spatio-temporal data structure.}
  \label{fig:data_mining/3D_structure}
\end{figure}

Inspired by Langlois et al. \cite{langlois2016inferring}, the x-y plane of our representation constructs a temporal structure between days of the month and hours of the day. The crucial advantage of this structure lies in its local properties. Similar to the case of Image Processing, in this representation a temporal pixel is simultaneously influenced by what happened in the previous/following hours (y axis), and on the previous/following days (x axis).

%TODO maybe redo image to reflect spatial, temporal and general
As for the z axis, each layer contains a trip attribute. In Figure \ref{fig:data_mining/3D_structure} boarding spatial attributes are portrayed in green, alighting spatial attributes are portrayed in red, and other types of general attributes (such as travel time, travel distance, transfer number, transfer total time, etc) are portrayed in yellow. 

Therefore, each temporal pixel may contain a trip feature vector, which expands several layers deep. Considering that even regular public transport users do not usually perform more than 6 trips a day, the proposed representation is sparse, since only a few time pixels are populated with trips.  


\subsection{Dimensionality reduction}
The proposed representation is directly proportional to the number of attributes used to describe a trip. As it is explain in Section \ref{sec:attributes}, we extract 26 attributes in a trip. Considering we have 30 days worth of data, and 24 hourly time bins in a day, this leads to $24 \times 30 \times 26 = 18,790$ temporal pixels to represent one user. Given the high dimensionality and the sparsity of the structure, we need perform dimensionality reduction in order to avoid the typicl effects of the curse of dimensionality. 

\subsubsection{Feature selection}
One of the simplest ways for reducing the number of attributes in a dataset is to perform feature selection. This technique consists of evaluating each feature's influence in making predictions or classifying samples. The strongest features are then selected to be part of the final dataset, while the least informative or redundant features are disregarded. 

When performing feature selection, there are two main choices to be made: the amount of features to be kept and the approaches to evaluate each feature. In this project, the first aspect is tackled using the \textit{Best k} algorithm, which fixes the amount of features to be kept to \textit{k}, regardless of the initial number of features present in the data. The second aspect is tackled via an assortment of algorithms, from statistical tests -such as correlation tests, and ANOVA f-test-, machine learning techniques -such as Trees classifiers-, to domain knowledge from Transportation domain specialists. 

\subsubsection{Feature extraction}
An alternative way of performing dimensionality reduction is by mapping high dimensional spaces to a low dimensional space. Though in most cases the mapping causes some loss of information, there exists mathematical transformations optimized for reducing the loss. Two of the most common techniques to do so are performing single value decomposition, as used by PCA, or work under the Universal Approximation Aroblem, as neural networks. 

In this project, we perform the mapping through an autoncoder. Autoencoders follow the principles of neural networks, and focus on the task of encoding and decoding an image minimizing the loss but not abstracting the noise. As a network, autoencoders may become as complex as needed, allowing for any number of layers and any types of activation functions. 

Taking advantage of the local properties of the proposed representation, we decide to apply stacked convolutional filters followed by a final fully conencted layer that outputs features of a more manageable dimensionality. The end result will be used as features for clustering commuters in Part III (Section \ref{sec:partIII}) of this project. 

\subsection{Pattern recognition}

\subsubsection{Ensemble models for classification}
There is a large variety of algorithms for performing classification tasks. Each algorithm may focus on different samples when learning the task, and so it has strengths and weaknesses different from one another. 

Ensemble models explore the idea of combining several non-correlated prediction methods that might correct each other in order to reach a better classification accuracy. Furthermore, ensemble models are robust and modular. Therefore, starting from a few weak classifiers, assembled via aggregation methods, the model can grow larger or more complex as needed.

This project constructs an ensemble model for classifying commuters. As proven by Tu \cite{tu2016impact}, weak classifiers, like a Support Vector Machines (SVM), are sufficient to identify commuting behavior up to a 94\% accuracy. Aiming to increase accuracy, but preventing overfitting, this project extends Tu's model with other similar weak classifiers such as decision trees, Bayesian classifiers, Gaussian Processes, multilayer perceptrons, and others. Their individual predictions are ensembled via the majority vote rule. 

\subsubsection{Clustering} 
For the third part of this project we perform behavior clustering. To this end, we apply the \textit{K means} algorithm on low dimensional features extracted by a convolutional autoencoder. The \textit{K means} algorithm is the standard algorithm used in the Transportation domain, and it consistently achieves satisfactory results. 

\subsection{Big Data considerations}
The amount of data gathered for this project requires specific computational resources. While programming the learning tasks, measures are taken to optimize the code. For example, the code is written in modules that work with one day file at the time, and save checkpoints at every relevant step. Additionally, the code is compatible with cluster computing and SLUM jobs are created for its processing.

Furthermore, parallel computing techniques are implemented wherever the whole dataset goes into memory, for example when extracting trip attributes from the records. As shown in Figure \ref{fig:bigdata/parallel}, the time required for applying a single filter reduces drastically if the number of parallel workers equals the number of cores in the machine used. 

\begin{figure}[H]
  \centering
  \includegraphics[width=.9\linewidth]{"./images/Parallel implementation"}
  \caption{Performance in parallel computing.}
  \label{fig:bigdata/parallel}
\end{figure}

The computational resources are provided by Vrije Universiteit Amsterdam. The results of this project are obtained by using DAS5 (The Distributed ASCI Supercomputer 5) \cite{bal2016medium}. 

\newpage
\section{Data preparation and preprocessing}
\label{sec:partI}
The data acquired for this project corresponds to November 2015. In total, 219,452,319 trip records from 15,437,796 public transit users are collected for the month. Figure \ref{fig:preprocessing/volume} shows the volume of records per day, color coded by day of the week. 

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{"./images/Daily volume of records"}
  \caption{Volume of raw data.}
  \label{fig:preprocessing/volume}
\end{figure}

The figure illustrates that, in general, Saturday and Sunday have less records compared to their corresponding weekdays. For example, for the first week of the month corresponding to days between the 2nd and the 8th, the weekend days (7th and 8th) have only about 4.5 million records, while the weekdays (from the 2nd to the 6th) have at least 6.5 millions each. The fourth week of the month, from the 23rd to the 29th, shows similar behavior with weekends having 5.5 million records but weekdays surpassing 8.5 million records.

Yet, the pattern for the third week of the month is more complex. While the weekends (21st and 22nd) have less records than the previous couple weekdays (Thursday 19th and Friday 20th), the first three weekdays are not as busy, with Monday having only 5 million records.

With regards to the second week of the month, from this plot we note a gap of six days with 0 records collected. Due to defective collection methods, the data from November 9th to November 14th is not available. 

The figure also shows an increase of public transit usage between Thursday 19th and Wednesday 25th. According to domain expert PhD. Liang Qu, the heavy usage is explained by weather conditions. During those days the weather was rainy and snowy; therefore there were more passengers than usual days.

Overall, it takes 45 minutes to preprocess and create the labeled dataset. It takes 48 hours to preprocess and create the reduced unlabeled dataset.

\subsection{Cleaning}
As first step for preparing the data for its mining, we eliminate faulty records. To this end, we apply the following four filters:

\begin{enumerate}
\item Eliminate records with empty fields: \textasciitilde 7.26\% records eliminated
\item Eliminate records with incomplete travel details: \textasciitilde 0.76\% records eliminated
\item Eliminate records with travel time $<= 0$: <0.01\% records eliminated
\item Eliminate records with travel distance $<= 0$: \textasciitilde 9.06\% records eliminated
\end{enumerate}

\begin{figure}[h]
  \centering
  \includegraphics[width=.8\linewidth]{"./images/faulty data"} %TODO run on whole
  \caption{Reasons for eliminating records.}
  \label{fig:preprocessing/faulty}
\end{figure}

The percentage of data eliminated by each filter is shown in Figure \ref{fig:preprocessing/faulty}. Removing faulty data reduces the dataset to \textasciitilde 82.93\% of its original size. After cleaning, the dataset contains 182,033,638 trip records.  %595932 card codes

\subsection{Extraction}
From the raw records, we can further extract attributes to complement the trip information. For example, taking the date of the travel we can further obtain two important attributes. The "Day" attribute, corresponding to the number of day on the month, from 1 to 31. The "Weekday" attribute is a number from 1 to 7 corresponding to the day of the week, starting on Monday.

\begin{figure}[H]
  \centering
  \includegraphics[width=.7\linewidth]{"./images/Number of trips"}
  \caption{Number of trips distribution.}
  \label{fig:preprocessing/num_trips}
\end{figure}

Furthermore, we calculate the "Number of trips" attribute which counts the trip records each user has in a day. Figure \ref{fig:preprocessing/num_trips} shows the distribution of this attribute on a random weekday. We note that most people perform two trips per day, followed by people who perform only one trip a day. 

Other fields, such as "Start/End time" and "Trip details" require more specialized algorithms for attribute extraction.

\subsubsection{Time bins}
Using hourly time bins is standard practice in the field and has proven sufficient to examine temporal data \cite{langlois2016inferring} \cite{ma2017understanding} \cite{morency2007measuring}. Therefore, in this project we follow the same technique and extract the hour of the start and end of each trip.

\begin{figure}[H]
  \centering
  \includegraphics[width=.7\linewidth]{"./images/Hour of trip"}
  \caption{Distribution of start/end hours for trips.}
  \label{fig:preprocessing/start_end_hour}
\end{figure}

From Figure \ref{fig:preprocessing/start_end_hour}, we note that our data follows the expected distribution for the domain, showing clear morning and evening peaks. Furthermore, we note that boarding and alighting patterns during the peaks hours are shifted by one hour. For example, the morning peak for boarding is between 7:00 am and 8:00 am, while the alighting peaks at 8:00 am and 9:00 am. Similarly, the evening peak happens between 5:00 pm and 6:00 pm for boarding, but between 6:00 pm and 7:00 pm for alighting. 

\subsubsection{Trip parsing} 
\label{sec:tripParsing}
The trip details obtained from the records are in Chinese, with descriptors containing a combination of numbers and text. In order to extract boarding/alighting route features, the descriptors must be parsed. We parse the trip details using a combination of two techniques: regular expressions and the construction of an ID vocabulary. 

\textbf{Regular expressions}

Since a trip may include transfers, we define a trip to be composed of one or many rides. Each ride is carried out in a single travel mode. In order to obtain the elements of each ride we look at the descriptor pattern according to its travel mode.

    \begin{align*}
    BIKE &= (bike.STOP-STOP) \\
    SUBWAY &= (subway.LINE:STOP-LINE:STOP) \\
    BUS &= (bus.ROUTE(FROM-TO):STOP \\
    &-ROUTE(FROM-TO):STOP)
	\end{align*}
	
where the upper-case text corresponds to placeholders for ride elements, the lower-case text corresponds to the English translation of the descriptor in Chinese, and the punctuation (parentheses, dots, colons and dashes) correspond to separators between ride elements.

Unifying the mode-specific patterns, we describe a ride and a trip using regular expressions:
    
	\begin{align*}	        
    RIDE &= (MODE.[LINE/ROUTE:]?STOP-[LINE/ROUTE:]?STOP) \\
    TRIP &= RIDE[->RIDE]? 
	\end{align*}    
	
where elements surrounded by squared brackets and followed by a question mark (e.g. $[ELEMENT]?$) correspond to optional elements. Note that when parsing bus details, we disregard the route direction. This decision is motivated to fit both subway lines and bus routes to a single pattern, acknowledging that the direction of the route does not affect the path of the route itself.

\textbf{ID Vocabulary}

Once the elements of a trip are extracted, they must be labeled by unique numerical IDs. These IDs are not available from the TOCC, hence we label them using our own system. IDs are assigned incrementally by 1, meaning that the next available ID is equal to the size of the vocabulary. Furthermore, IDs start from number 1 since the value 0 has a special meaning in the proposed representation.

\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{.7\textwidth}
  	\centering
  	\includegraphics[width=.9\linewidth]{"./images/Cummulative lines vocabulary size"}
  	\caption{Vocabulary containing subway lines and bus routes}
  \end{subfigure}
  \begin{subfigure}[b]{.7\textwidth}
  	\centering
  	\includegraphics[width=.9\linewidth]{"./images/Cummulative stops vocabulary size"}
  	\caption{Vocabulary containing subway and bus stops, as well as bikes drop off spots.}
  \end{subfigure}
  \caption{Cummulative plots.}
  \label{fig:preprocessing/cummulativeVoc}
\end{figure}

Two different vocabularies are created, the first one for subway lines/bus routes, and the second one for stops. Modes of transportation are assigned ID 1 for subway, ID 2 for bus and ID 3 for bicycle. 

Usually, bus routes are identified by a number. However, in Beijing a single bus route number can be associated to different paths. For example, night buses, express buses and other special cases of a bus route may follow different paths even if they are described with the same number. For this reason we create a vocabulary with all unique parsed routes according to their full description and not only their number. 

Vocabularies are created from the available data. As such, the first time a line/route/stop is seen it is assigned the next available numerical ID. Thus, the vocabulary is dependent on the order on which the records are processed. Moreover, the size of the vocabulary increases as we process more daily files. Figure \ref{fig:preprocessing/cummulativeVoc} shows the size of each of the vocabularies. We find  \textasciitilde 840 lines and \textasciitilde 6740 stops. Though different in size, both vocabularies exhibit the same growing behavior. 92\% of lines and 97 \% of stops are seen since on the first file, and by the 7th file we have seen 99\% of both vocabularies.

Examples of parsed routes for each mode of transportation are shown in Figure \ref{fig:preprocessing/parsed_routes}. 

\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{\textwidth}
  	\centering
  	\includegraphics[width=\linewidth]{./images/details_bus.png}
  	\caption{Bus route}
  \end{subfigure}
  \begin{subfigure}[b]{.75\textwidth}
  	\centering
  	\includegraphics[width=\linewidth]{./images/details_subway.png}
  	\caption{Subway route}
  \end{subfigure}
    \begin{subfigure}[b]{.7\textwidth}
  	\centering
  	\includegraphics[width=\linewidth]{./images/details_bike.png}
  	\caption{Bike route}
  \end{subfigure}
  \caption{Examples for parsed and tokenized trip details.}
  	\label{fig:preprocessing/parsed_routes}
\end{figure}


\subsection{Data patching}
\label{sec:patching}
We note that the number of transfers and the path link fields of some records do not correspond to the information in their trip details. According to domain expert PhD. Tu Qiang, this must be recalculated \cite{tommy}. 

\begin{figure}[H]
  \centering
  \includegraphics[width=.7\linewidth]{"./images/Number of transfers"}
  \caption{Transfer number distribution before and after recalculation.}
  \label{fig:preprocessing/num_transfers}
\end{figure}

Figure \ref{fig:preprocessing/num_transfers} shows the distribution of the number of transfers per trip before and after patching, illustrating that there is a significant correction in the distribution. The patched distribution shows that most trips are performed without transfers, which is consistent with other studies findings \cite{bhaskar2015passenger}.

\subsection{Standardization} 

\textbf{Continuous values}

In data mining, it is a standard practice to perform whitening. This technique eliminates correlations between features, which is desirable in most cases. However, for the domain of Metropolitan Transportation some of these correlations are highly important and should not be discarded. This is the case of total travel time and distance, as shown in Figure \ref{fig:preprocessing/distance_time_correlation}. 

Most trips follow a linear correlation between distance and time. Trips above the linear axis correspond to users traveling short distances over relatively long times (i.e. traffic jams). Trips below the linear axis correspond to users traveling large distances over relatively short times (i.e. express trips). Though there exist some outliers, most of the data follows the same distribution. For this reason, we choose to only standardize the attribute but keep the correlations. 

\begin{figure}[H]
  \centering
  \includegraphics[width=.7\linewidth]{"./images/distance vs time"}
  \caption{Travel distance vs travel time.}
  \label{fig:preprocessing/distance_time_correlation}
\end{figure}

Travel time, travel distance, total transfer time and average transfer time are standardized by subtracting the mean of each distribution and forcing a unit standard deviation. We standardize every daily file separately, therefore preserving each day's characteristic distribution. This allows us to maintain distinct days separate, such as weekdays and weekends.

\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{.45\textwidth}
  	\centering
  	\includegraphics[width=\linewidth]{"./images/Travel time standardized"}
  	\caption{Travel time}
  \end{subfigure}
  \begin{subfigure}[b]{.45\textwidth}
  	\centering
  	\includegraphics[width=\linewidth]{"./images/Travel distance standardized"}
  	\caption{Travel distance}
  \end{subfigure}
  \caption{Time and distance standardize distributions.}
  	\label{fig:preprocessing/timeDist}
\end{figure}

Figure \ref{fig:preprocessing/timeDist} shows distributions for travel time and distance. Since the nature of the data prevents negative values (time and distance must be positive), the original distribution is truncated at 0. Standardization maintains the shape of the distribution, but shifts and contracts it to be closer to zero values.

The mean travel time is \textasciitilde 35.18 minutes for weekdays and \textasciitilde 36.56 minutes for weekends. The mean travel distance is \textasciitilde 14.37 kilometres for weekdays and \textasciitilde 15.36 kilometres for weekends. These values are relatively small for a busy city such as Beijing, with constant traffic jams. We believe that bikes (commonly used for short distances and short periods of time) and the efficiency of its subway system helps to alleviate the stress over the mean travel time and distance. 

%TRAVEL_TIME          2.11123695238 / 2.1936035e+06, 
%TRAVEL_DISTANCE      1.4378304619 / 1.53654675e+04, 
%TRANSFER_TIME_AVG    9.023386e+04, TRANSFER_TIME_SUM    1.017949e+05

\textbf{Categorical values}

As mentioned in Section \ref{sec:structure}, a trip contains both continuous and categorical attributes. Generally, categorical values must to be transformed into one hot encoding. This prevents problems with ordinal values which impose underlying structures on the data.

In the context of this project we find that a trip contains 12 categorical attributes. Furthermore, each of these contains its own range, with some of them having up hundreds or thousands of categories (i.e. Middle and Small traffic areas). For these reasons, one hot encoding is not a scalable option for this project.


\subsection{Attributes}
\label{sec:attributes}
After the data is preprocessed, we collect 26 attributes that describe a trip. We divide them onto three categories: general attributes, temporal attributes and spatial attributes. 

The general trip attributes are: 

\begin{enumerate}
\item Number of day
\item Weekday
\item Number of trips
\item Travel time
\item Travel distance
\item Number of transfers
\item Transfer total time
\item Transfer average time
\end{enumerate}

The temporal trip attributes are:
 
\begin{enumerate}
\item Start hour
\item End hour
\end{enumerate}

Finally, the spatial attributes are:

\begin{enumerate}
\item On/Off District
\item On/Off Small traffic area
\item On/Off Middle traffic area
\item On/Off Big traffic area
\item On/Off Ring road
\item On/Off Mode
\item On/Off Line/Route
\item On/Off Stop
\end{enumerate}

\subsection{User cubes}
\label{sec:userCubes}
Figures \ref{fig:preprocessing/cubesCom} and \ref{fig:preprocessing/cubesNonCom} show the first slice of four user's personal traveling behavior representation. The slices correspond to the attribute "Number of day" and show the temporal distribution of their trips over the month. A dark square indicates a trip was made, or rather started, on $x$ day during $y$ hour. A light square implies there was no recorded public transit activity during that time. Two random Commuters and two random Non-commuters are shown. 

For the case of Commuters, Commuter 1 is a heavy public transit user. For the first week, between Monday 2nd and Friday 6th, he makes fours subsequent morning trips every day at the same time. He also travels by public transit on two evenings this week, though at different times. His usage decreases during the third week, having only one morning trip on Thursday 26th and three evening trips, one on Thursday 26th and two on Friday 27th. The busiest usage of public transit comes over the fourth week, between Monday 23rd and Friday 27th. This time the user performs his morning trip every day of the week, and travels at least once every evening, with Monday 23rd showing three evening journeys. Interestingly, this user does not show any public transit usage over the weekends. In summary, Commuter 1 shows a clear pattern on his trips distribution. It suggests a semi-flexible working schedule with fixed entry time but variable exit time. 

Compared with Commuter 1, Commuter 2 is a moderate user of the public transit network. On the third week, between Monday 16th and Friday 20th, we observe a couple trips at 6 in the morning and a couple more at 5 in the evening, suggesting a typical working schedule. Similarly, during the fourth week, the user presumably travels to work by public transit only once, on Friday 27th, and comes back home by public transit twice, on Thursday 26th and Friday 27th. He also uses public transit on the weekends, twice on Sunday 1st and twice on Sunday 15th.

\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{.45\textwidth}
  	\centering
	\includegraphics[width=\linewidth]{"./images/Commuter 1 Day"}
  	\caption{Random Commuter 1.}
  \end{subfigure}
  \begin{subfigure}[b]{.45\textwidth}
  	\centering
	\includegraphics[width=\linewidth]{"./images/Commuter 2 Day"}
  	\caption{Random Commuter 2.}
  \end{subfigure}
  \caption{Sample cube slices for Commuters.}
  	\label{fig:preprocessing/cubesCom} 
\end{figure}

Non-commuters, in contrast, demonstrate an irregular distribution of trips. Non-commuter 1 has one busy day on Monday 2nd, where he performs three trips on one day between 10 in the morning and 3 in the afternoon. Beyond that, he is only active three more days over the whole month, Tuesday 3rd, Wednesday 4th and Thursday 26th, with only one journey on each day. The only discernible pattern  of this user is that he only travels over weekdays.

Analogously, Non-commuter 2 is only active 5 days over the whole month, Monday 1st, Tuesday 2nd, Monday 23rd, Sunday 29th and Sunday 30th. Contrary to Non-commuter 1, Non-commuter two travels on weekdays and weekends with no apparent pattern of behavior.

\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{.45\textwidth}
  	\centering
	\includegraphics[width=.9\linewidth]{"./images/Non-commuter 1 Day"}
  	\caption{Random Non-commuter 1.}
  \end{subfigure}
  \begin{subfigure}[b]{.45\textwidth}
  	\centering
	\includegraphics[width=.9\linewidth]{"./images/Non-commuter 2 Day"}
  	\caption{Random Non-commuter 2.}
  \end{subfigure}
  \caption{Sample cube slices for Non-commuters.}
  	\label{fig:preprocessing/cubesNonCom} 
\end{figure}

It is important to note that as the data between Monday November 9th and Saturday November 14th is missing. Thus, both cases show a gap in records during these days. However, we note that the gaps are not restricted to these days, and that the behavior is very irregular, even for some cases labeled as commuter such as Commuter 1. 

\newpage
\section{Commuters identification}
\label{sec:partII}
In this part of the project we focus on the task of classifying public transit users into one of two categories: Commuter or Non commuter. To do so, examine the relevance of each pf the trips attributes and we reduce the dimensionality of the original user cubes by selecting the most informative feature slices. Subsequently, several weak classifiers are trained and evaluated. The best classifiers are selected and incrementally added to an ensemble model.The final model is selected according to the highest accuracy achieved.

\subsection{Attributes correlation} 
First, we perform an exploratory study of the relationships among all the attributes of a trip. We calculate the correlation matrix of the trip attributes using the labeled dataset refer to in Section \ref{sec:datasets}. The set contains 12,584 trips corresponding to 639 labeled card codes. These correspond to 346 Commuters, and 293 Non-commuters, as labeled by a self reported survey. 

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{"./images/General"}
  \caption{Attributes correlation to each other and to label. Color bar indicates the direction and magnitude of correlation.}
  \label{fig:classification/correlation}
\end{figure}

From Figure \ref{fig:classification/correlation} we note distinguishable blocks with specific patterns. We perform an analysis by attribute category in order to better understand the interdependence among attributes.

\textbf{General} 

It is clear that travel time and travel distance are highly correlated. Rationally, this may be explained by the reduced offer for express trips in the public transit network of Beijing. As a counterexample, in Amsterdam it is possible to travel significant distances within the city in a fast manner by using the train. Alternatively, the correlation can also be explained by the vast public transit network in Beijing. Given the high inter-connectivity and large offer of routes, it is possible to travel from one point to another in a relatively straight way, thus avoiding detours or requiring to travel to major stations for transfers. 
 
Furthermore, the transfer average and total times are also highly correlated. As examined in Section \ref{sec:patching}, most trips are have one transfer, which makes the relationship between average and total transfer time linear.

\textbf{Temporal}

In this block we see a very high positive correlation, represented by a dark red color. This indicates that the start and end hour for most trips are identical. Given that most trips are completed within the hour, this is coherent the data exploratory findings. 

\textbf{Spatial}

With regards to the traffic areas and the ring road areas, we note that the boarding attributes share a stronger connection among themselves than with their correspondent alighting attributes. Since the traffic areas are hierarchical (small, middle and big divisions), it is logical for them to be correlated. Similarly, the ring road areas, though having a slightly different division, maintain a fixed relationship with the traffic areas and thus one can be inferred or approximated from the other. 

Finally, we observe that the boarding and alighting mode of transportation are highly correlated, suggesting that most trips are performed using only one mode of transportation. 

\subsection{Feature selection}
Feature selection is the process of choosing features that aid in the learning task at hand, and disregarding the information that is not helpful. From the correlation matrix we note that some attributes are redundant. Hence, feature selection can help to disregard those which are the least informative and reduce the dimensionality.

We evaluate each attribute via its correlation to the true class label, its importance according to an ExtraTrees classifier, its ANOVA f-value, and the jugdment of usefulness according to domain expert PhD. Liang Qu. Initially, a Chi Squared test was also performed, but was abandoned since it had significant bias in favor of continuous attributes over categorical attributes. 

Each method's scores are normalized to sum up to 1. Then, we aggregate them and calculate the final score. The results are shown in Figure \ref{fig:classification/scores}. 

\begin{figure}[H]
  \centering
  \includegraphics[width=.95\linewidth]{"./images/scores"}
  \caption{Attributes scores.}
  \label{fig:classification/scores}
\end{figure}

For the final set selection, we take a fixed amount of attributes from each category, in order to maintain a balanced set. 

\begin{itemize}

\item From the \textit{General attributes}, we select the best 3 out of 8: Number of trips, Weekday, and Travel time. 

\item Given that the size of the set is already reduced, we keep all the \textit{Temporal attributes}. Formally speaking, we select the best 2 out of 2: Start hour and End hour. 

\item For the \textit{Spatial attributes}, we design a system for pair selection. This mechanism ensures that if an attribute is selected, its boarding/alighting counterpart is also selected. As such, we select the best 2 out of 8: On/Off mode, and On/Off line. The cubes are flatten in order for them to be fed into different weak classifiers. 

\end{itemize}

A final set of 9 features is created. The cube slices corresponding to the selected features are kept in the user cubes and the rest disregarded, resulting in \textasciitilde 66\% reduction. 

\subsection{Model} 
As suggested by Tu's results \cite{tu2016impact}, the data is almost linearly separable. Thus, weak classifiers such may suffice to achieve a satisfactory accuracy (chance prediction is 50\%).

We train four weak classifiers: one linear SVM, one Gaussian Process, one Gaussian Naive Bayes classifier, and one multilayer perceptron. We also train three simple ensemble models based on decision trees: a random forest, a model using the adaboost algorithm, and a model using bagging. The parameters for each classifier are shown in Table \ref{table:classifiersParam}

\begin{table}[H]
\centering
\begin{tabular}{||c|c||}
\hline
\textbf{Classifier} & \textbf{Parameters} \\ [0.5ex] 
\hline \hline
SVM & Linear kernel, One vs all decision function \\
Gaussian process & RBF kernel\\
Gaussian Naive Bayes & Prior probabilities set according to data \\
Multilayer perceptron & 100 layers, ReLu activation function \\
Random Forest & 100 trees, max depth of 10 \\
Adaboost of decision trees & 100 trees \\
Bagging of decision trees & 100 trees\\ [1ex]
\hline 
\end{tabular}
\caption{Accuracy for weak classifiers}
\label{table:classifiersParam}
\end{table}

The seven weak classifiers are trained and evaluated separately. The dataset to be used contains 639 user cubes to be classified into one of the two categories. We flatten the reduced user cubes since the classifiers require the data to be represented as vectors. For evaluation we split the dataset into 70\% for training and 30\% for testing, as evaluated by Tu. Furthermore, we evaluate using more robust techniques, such as cross validation over 5 folds. 

We proceed to build an ensemble model based on a Majority Vote aggregation method. This method takes the prediction from each individual classifier, and select the class which was predicted by the most classifiers. When facing a tie among classes, the model selects the class based on ascending order. Since we perform binary classification, that means the default category is Non-Commuter. 

Additionally, we compare models using "hard" voting and "soft" voting. "Hard" voting refers to taking each classifier class prediction, while "soft" voting works under probabilistic predictions. 

Incrementally, we incorporate the best $k$ classifiers and evaluate at each step. The final model is selected according to the largest accuracy using cross validation over 5 folds. 

\subsection{Experiments} 
The weak classifiers performance is reported by its weighted mean accuracy over both classes. We report two experiments: a) a single 7:3 train/test division of the dataset, and b) average of cross validation over 5 folds. Each experiment is run 6 times, thus we report the average of each condition. Table \ref{table:weakClassifiers} summarizes the results. 



\begin{table}[H]
\centering
\begin{tabular}{||c|c|c||}
\hline
\textbf{Classifier} & \textbf{Single run accuracy} & \textbf{Cross validation accuracy} \\ [0.5ex] 
\hline \hline
SVM & 75.0\% & 62.0\% (+/- 4.0)\\
Gaussian process & 77.3\% & 45.0\% (+/- 0.0)\\
Gaussian Naive Bayes & 76.8\% & 66.0\% (+/- 7.0)\\
Multilayer perceptron & 78.0\% & 64.6\% (+/- 4.5)\\
Random Forest & \textbf{84.3\%} & \textbf{74.6\% (+/- 6.5)}\\
Adaboost of decision trees & 79.6\% & 68.5\% (+/- 5.6)\\
Bagging of decision trees & 82.6\% & 73.0\% (+/- 8.0)\\
\hline 
\end{tabular}
\caption{Accuracy for weak classifiers}
\label{table:weakClassifiers}
\end{table}

With 74.6\% accuracy over cross validation sets, Random Forest is the strongest classifier. This is reasonable since Random Forests are generally competent when dealing with categorical information. Given that 6 out of the 9 trip features we kept after feature selection are categorical, this method adapts smoothly to our data. 

The incremental ensemble model performance is reported by its average accuracy using cross validation over 5 folds. We perform two experiments: a) ensemble via "hard" voting, and b) ensemble via "soft" voting. Each experiment is run 3 times. Average results across runs are shown in Table \ref{table:ensembleModels}. 

\begin{table}[H]
\centering
\begin{tabular}{||c|c|c|c||}
\hline
\textbf{\# classifiers} & \textbf{Classifiers included} & \multicolumn{2}{|c|}{\textbf{Cross validation accuracy}} \\ \cline{3-4}
 &  & \textbf{Hard voting} & \textbf{Soft voting} \\ [0.5ex] 
\hline \hline
Best 2 & {Bagging of decision trees, Random forest} & \textbf{75.0\% (+/- 8.0)} & \textbf{73.6\% (+/- 7.3)}\\
Best 3 & Previous set $+$ {Adaboost of decision trees} & 73.6\% (+/- 8.0) & \textbf{73.6\% (+/- 7.6)}\\
Best 4 & Previous set $+$ {Gaussian Naive Bayes} & 73.3\% (+/- 7.6) & 72.0\% (+/- 6.6)\\
Best 5 & Previous set $+$ {Multilayer perceptron} & 73.3\% (+/- 7.3) & 72.6\% (+/- 7.6)\\
Best 6 & Previous set $+$ {SVM} & 73.0\% (+/- 7.3) & 73.0\% (+/- 6.3) \\
Best 7 & Previous set $+$ {Gaussian Process} & 73.0\% (+/- 6.6) & 72.0\% (+/- 6.0) \\ 
\hline 
\end{tabular}
\caption{Accuracy for incremental ensemble model. Comparing "hard" and "soft" voting.}
\label{table:ensembleModels}
\end{table}

We observe that "hard" voting achieves equal or better performance than "soft" voting for all models. The largest difference is 1.4\% with regards to the \textit{Best 2} model. We believe this is due to the tie rule (which favors Non-commuters) occurring more frequently while performing "hard" voting. 

The \textit{Best 2} model obtains the best performance in both conditions. However, while performing "soft" voting, the \textit{Best 3} model matches its performance though with lower robustness. Additionally, in one out of the three runs, the \textit{Best 3} model was chosen over \textit{Best 2}, given "soft" voting. Yet, even in that case, the \textit{Best 3} model achieved 74.0\% (+/- 8.0), only 1\% above than the \textit{Best 2} model with 73.0\% (+/- 7.0). 



\begin{table}[H]
\centering
\begin{tabular}{||c|c|c||}
\hline
\textbf{Model} & \textbf{Cross validation accuracy} & \textbf{Type of model} \\ [0.5ex] 
\hline \hline
Best 2 & 75.0\% (+/- 8.0) & Complex ensemble model \\
Random Forest & 74.6\% (+/- 6.5) & Simple ensemble model \\
Best 3 & 73.6\% (+/- 8.0) & Complex ensemble model \\
Best 5 & 73.3\% (+/- 7.3) & Complex ensemble model \\
Best 4 & 73.3\% (+/- 7.6) & Complex ensemble model \\
Best 7 & 73.0\% (+/- 6.6) & Complex ensemble model  \\
Best 6 & 73.0\% (+/- 7.3) & Complex ensemble model \\
Bagging of decision trees & 73.0\% (+/- 8.0) & Simple ensemble model \\ 
Adaboost of decision trees & 68.5\% (+/- 5.6) & Simple ensemble model \\
Gaussian Naive Bayes &66.0\% (+/- 7.0) & Weak classifier \\
Multilayer perceptron & 64.6\% (+/- 4.5) & Weak classifier \\
SVM & 62.0\% (+/- 4.0) & Weak classifier \\
Gaussian process & 45.0\% (+/- 0.0) & Weak classifier \\
\hline 
\end{tabular}
\caption{Models and their type, ranked by accuracy}
\label{table:rankingModels}
\end{table}

Furthermore, the results show that including more classifiers does not necessarily boost performance, but it may even affect it negatively. For example, adding the Gaussian Naive Bayes classifier to the set of 3 indeed reduces accuracy on both voting conditions. Taking a closer look into the Naive Bayes classifier, we see that its performance is the least stable when trained over only one train/test set, ranging from 69\% to 82\%. Therefore, we suspect that its inclusion brings noise to the model overall. 

Regardless, ensemble models perform better than weak classifiers alone, as shown in Table \ref{table:rankingModels}. Complex ensemble models refer to models that mix different types of weak classifiers, while simple ensemble models refer to models based on decision trees only. 

The model with highest accuracy corresponds to combining the best 2 classifiers: Random forest and Bagging of decision trees. However, the improvement of this ensemble model over the Random Forest classifier alone is only 0.4\%. In order to investigate how the two selected classifiers complement each other, we take a look at their confusion matrices. 

\begin{figure}[H]
  \centering
  \includegraphics[width=.80\linewidth]{"./images/Random forest"}
  \caption{Confusion matrix for Random forest classifier.}
  \label{fig:classification/rf_confusion}
\end{figure}


\begin{figure}[H]
  \centering
  \includegraphics[width=.80\linewidth]{"./images/Bagging of decision trees"}
  \caption{Confusion matrix for classifier based on bagging decision trees.}
  \label{fig:classification/bag_confusion}
\end{figure}

Interestingly, both classifiers have a similar behavior. Figures \ref{fig:classification/rf_confusion} and \ref{fig:classification/bag_confusion} show that both perform well in identifying commuters. Random forests achieves 70.19\% closely outperformed by Bagging with 74.03\%. Similarly, they both perform even better when identifying Non-commuters, with 88.6\% and 80.68\% correspondingly. 

The high level of similarity between both classifiers explains the low increase in performance that the ensemble model has. While the Random Forest classifier is very good at identifying both classes, the Bagging of decision trees classifier aids to increase the identification rate for Commuters. However, the help it provides is diluted due to the tie rule. In summary, in order to correctly identify a commuter both classifiers must predict the class correctly, otherwise the label with be Non-commuter by default.

\subsection{Discussion}

\textbf{Dataset and representation}


With the goal in mind of comparing our work to Tu's \cite{tu2016impact}, we selected the same training parameters for the linear SVM classifier. However, Tu's results could not be replicated. We consider that this is a consequence of using a different dataset for training. 

In terms of quantity, Tu's dataset was \textasciitilde 150\% larger, having 978 labeled user card codes against ours with 639 labeled user card codes. Furthermore, the labels for user card codes were gathered during January 2015. Presumably, some users might have altered their behavior by changing their traveling patterns or completely stopped being active public transit users by November 2015. This brings noise to the labels in the dataset.

In terms of representation, Tu's dataset presents a simpler representation, including only one temporal and two spatial features for one week of public transit usage. In contrast, our representation includes a mixture of general, temporal, and spatial features gathered over a month. As a result, we suspect that that the dataset is not linearly separable anymore. 

In order to investigate this hypothesis we visualize both representations using two dimensional t-Distributed Stochastic Neighbor Embedding (TSNE). 
  
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{"./images/Tu's features"}
  \caption{Visualization of Tu's samples under their vectorized representation.}
  \label{fig:classification/tu}
\end{figure} 

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{"./images/Selected features"}
  \caption{Visualization of samples and their labels after feature selection.}
  \label{fig:classification/tsne}
\end{figure}  

Figures \ref{fig:classification/tu} and \ref{fig:classification/tsne} show each representation's manifold. Tu's straightforward representation gathers most commuters in three separate clusters. Non commuters are spread along a larger cluster, with some commuters blended in.  In contrast, the reduced user cubes form only one cluster. Though slightly more commuters align in the lower part of the space, and non commuters are present in the upper part of the space, both classes are virtually merged. 

Thus, we confirm our hypothesis. It is clear that the classes are not linearly separable anymore, contrary to what was expected when starting the project. This suggests our representation and/or the dimensionality technique is not fit for binary classification.

\textbf{Ensemble models}

The findings of this part of the project confirm the hypothesis that an ensemble model performs significantly better than a linear SVM, as designed by Tu. However, a specific type of ensemble model is required to maximize performance. 

The final model consists of combining two simple ensemble models based on decision trees. This suggests that decision trees are ideal for our data due to its categorical nature. Furthermore, the model is ensembled by two levels. On a first instance, a large number of classifiers of the same type are aggregated (i.e. 100 decision trees). On a second instance, a small number of classifiers of a different type are aggregated (i.e. one random forest and one bagging model). The result leads to a 9\% increase in performance over any weak classifier alone.  

However, though the chosen ensemble model maximizes performance, the diversity between the selected classifiers is unsatisfactory. Both classifiers have similar behavior and performance on each class. Ideally, two classifiers that differ in their strengths would benefit each other better. 

In comparison the state of the art for commuters classification, our results fall short. Ma et al. \cite{ma2017understanding} achieve 94.1\% detection accuracy; however, their dataset is small, containing 118 labeled samples only. Tu et al. \cite{tu2016impact} report 94.24\% accuracy in detecting commuters. Yet, their evaluation technique contains only one test set thus making it prone to overfitting. Furthermore, we believe that the missing information in our dataset might have affected the classifiers performance, thus making it unsuitable for a fair comparison.

\newpage
\section{Traveling behavior clustering}
\label{sec:partIII}
In this part of the project we perform an unsupervised learning task to cluster public transit users according to patterns in their travel behavior. This time we perform dimensionality reduction by encoding the original user cubes onto a low dimensional space by means of an autoencoder. Then, we cluster the user features and analyze each cluster's characteristics. 

For this part of the project we use the reduced unlabeled dataset as described in Section \ref{sec:datasets}. The set contains 1,191,862 trips corresponding to 95,565 card codes. 

\subsection{Feature extraction}
As mentioned in Section \ref{sec:structure}, the proposed representation is sparse, hence dimensionality reduction has the potential to extract the relevant information and disregard noise. Furthermore, the proposed representation is local, in the same sense as images are. Combining these two characteristics, it is natural to think of convolutional filters as a tool to extract features from the user cubes. 

\subsubsection{Convolutional filters}
We apply two dimensional filters to the three dimensional user cubes. Therefore, the x and y dimensions of the filter align with the day and time dimensions of the structures. Similarly to images and RGB channels, each of the feature planes is considered a channel in the convolutional filter. 

The size of the filters is fixed to $3 \times 3$, since it leads to the best performance according state of the art techniques. The stride is set to 1 and we perform padding in order for the dimensionalities of the network to be suitable.

\subsubsection{Autoencoder}
Using Keras and Tensorflow as backend, we create an autoencoder. The network structure is shown below:

\begin{table}[H]
\centering
\begin{tabular}{||c|c|c|c|c|c||}
\hline
\textbf{Type of layer} & \textbf{Filter size} & \textbf{Filter units} & \textbf{Stride} & \textbf{Activation function} & \textbf{Output dimensions} \\
[0.5ex] 
\hline \hline
Input & None & None & None & None  & [24, 30, 26]\\
Convolutional & [3, 3] & 16 & 1 & ReLU  & [24, 30, 16]\\
Max Pooling & [2, 2] & 16 & 1 & None  & [12, 15, 16]\\
Convolutional & [3, 3] & 8 & 1 & ReLU  & [12, 15, 8]\\
Max Pooling & [3, 3] & 8 & 1 & None  & [4, 5, 8]\\
\hline
Convolutional & [3, 3] & 8 & 1 & ReLU  & [4, 5, 8]\\
Up sampling & [3, 3] & 8 & 1 & None  & [12, 15, 8]\\
Convolutional & [3, 3] & 16 & 1 & ReLU  & [12, 15, 16]\\
Up sampling & [2, 2] & 16 & 1 & None  & [24, 30, 16]\\
Convolutional & [3, 3] & 26 & 1 & Sigmoid  & [12, 15, 26]\\ [1ex]
\hline 
\end{tabular}
\caption{Autoencoder network structure}
\label{table:autoencoderStructure}
\end{table}

The first block corresponds to the encoder module, while the second block corresponds to the decoder module. The dimensionality of the encoded features is $4 \times 5 \times 8 = 160$. This represents more than 110\% compression from the original dimensionality. 

We evaluate the autoencoder using the binary crossentropy loss between the original user cube and the reconstructed user cube. As an optimizer we use Adadelta. We train on 60\% of the data and validate on 40\% of the data for 500 iterations. The loss over training and validation sets is shown in Figure \ref{fig:clustering/loss}.

We note that at around 200 iterations the validation loss begins to rise, closely followed by an increase in training loss. This is a sign for overfitting since the autoencoder is learning the specific representations for the samples in the the current batch of the training set, but loses generality on the rest of the training set and the validation set. Thus, the final autoencoder is trained for only 200 iterations.

\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{.9\textwidth}
  	\centering
	\includegraphics[width=.9\linewidth]{"./images/autoencoder loss"}
  \caption{Loss over training set.}
  \end{subfigure}
  \begin{subfigure}[b]{.9\textwidth}
  	\centering
	\includegraphics[width=.9\linewidth]{"./images/validation loss"}
  \caption{Loss over validation set.}
  \end{subfigure}
  \caption{Binary cross entropy loss against training steps in autoencoder.}
  	\label{fig:clustering/loss} 
\end{figure}

\subsection{Clustering}

\begin{figure}[H]
  \centering
  \includegraphics[width=.9\linewidth]{"./images/Tuning for k"}
  \caption{Silhouette average scores and their standard deviation.}
  \label{fig:clustering/tuning}
\end{figure}

The K means algorithm has only one parameter: the number of clusters in which to allocate the data. As such, this parameter must be properly tuned in order for the algorithm to produce valuable results. 

In order to tune \textit{K}, we split the data into train and test sets on a 6:4 ratio. Then, we test the performance on values for \textit{K} from 3 to 15, since we want to go beyond binary classification but we want to keep the number of clusters manageable. The training set serves the purpose for calculating the clusters centers. The test set aids in evaluating the quality of the clusters, according to how test samples are assigned. For evaluation purposes we calculate the silhouette score. 

The silhouette score is determined using the mean intra-cluster distance and the nearest-cluster distance (which the sample does not belong to). It ranges from 0 to 1, with higher scores corresponding to more condensed well-formed clusters. The score is measured per sample, and averaged over all the set. 

Figure \ref{fig:clustering/tuning}, shows the average score per \textit{K} value, as well as the standard deviation in the samples scores distribution. The average score peaks at $\textit{K} = 3 $. However, we would like to explore a larger set of behaviors. Thus, we choose the second highest peak at $\textit{K} = 7 $. We select 7 as the optimal number of clusters. 

\subsection{Cluster analysis}

\textbf{Qualitative analysis}


Figure \ref{fig:clustering/tsne} shows the samples, as visualized by two dimensional t-Distributed Stochastic Neighbor Embedding (TSNE), and their corresponding cluster label. Though not completely separable, there exists a manifold gathering similar samples close to each other. . 

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{"./images/Clustered"}
  \caption{Visualization of samples and their labels according to clustering. 5,000 sample.}
  \label{fig:clustering/tsne}
\end{figure}

\textbf{Quantitative analysis}

Table \ref{table:clusterDistribution} shows the distribution of samples over the different clusters. With regards to the users distribution, clusters are significantly unbalanced. For instance, Cluster 5 has around twice as many users as any other Cluster, except for Cluster 6. In fact, Cluster 5 is the largest user group containing more than 23\% of the users, while Cluster 3 is the smallest user group with 10.51\% or the users.

However, the trips distribution differs from the users distribution. Cluster 6 contains almost half of the trips for the month, even though it is not the largest user cluster. Therefore, we hypothesize that users in Cluster 6 are heavy users of the public transit. Comparably, Cluster 4 contains the least amount of trips but it is not the smallest user group. 

We compare the labels acquired by survey data to the labels assigned in clustering. We note that more than half of the Commuters (as labeled by survey data) are grouped in Cluster 6. In contrast, Clusters 1 and 4 contain the least amount of commuters. As for Non-commuters, most of them gather in Cluster 5, while Cluster 3 contains the least of them.

\begin{table}[H]
\centering
\begin{tabular}{||c|c|c|c|c|c|c|c||}
\hline
 & \textbf{C 0} & \textbf{C 1} & \textbf{C 2} & \textbf{C 3} & \textbf{C 4}  & \textbf{C 5} & \textbf{C 6} \\
[0.5ex] 
\hline \hline
Number of cards & 12133 & 10481 & 11715 & 100045 & 11823 & \textbf{22359} & 17009 \\
Percentage of cards & 12.69 & 10.96 & 12.25 & 10.51 & 12.37 & \textbf{23.39} & 17.79 \\
\hline
Number of trips & 70508 & 54552 & 177415 & 184850 & 44994 & 65671 & \textbf{593872} \\
Percentage of trips & 5.91 & 4.57 & 14.88 & 15.50 & 3.77 & 5.50 & \textbf{49.82} \\
\hline
Commuters [\%] & 5.09 & 3.68 & 13.59 & 14.16 & 3.68 & 7.08 & \textbf{52.69} \\ 
Non-commuters [\%] & 13.28 & 11.18 & 16.08 & 16.43 & 10.83 & \textbf{18.53} & 13.63 \\ 
Unknown [\%] & 12.72 & 10.99 & 12.24 & 10.47 & 12.40 & \textbf{23.47} & 17.68 \\  [1ex]
\hline 
\end{tabular}
\caption{Cluster Distribution} %distribution of samples over cluster
\label{table:clusterDistribution}
\end{table}

In order to look further into its differences, Table \ref{table:clusterAnalysis} analyses the characteristics of each cluster. We confirm our previous hypothesis, as Cluster 6 has the highest average number of trips, and Cluster 5 has the lowest average number of trips. Cluster 5 contains the longest trips, also corresponding to the longest traveled distance. Cluster 2 groups users who perform the shortest trips, and the least average traveled distance. 

With regards to the modes of transportation, most trips have bus as its boarding and alighting modes of transportation. Thus, most clusters have a high percentage of Bus - Bus trips. However, Cluster 6 is the exception as it mostly contains Metro - Metro trips. 

Cluster 0 contains trips with the least amount of transfers. We also note that trips with only one mode of transportation (Metro - Metro, Bus - Bus, and Bike - Bike) account for 92.88\% of its trips.

The largest usage of bus trips are gathered in Cluster 1. More than 61\% of trips performed by its users are performed by bus only, and 67.50\% of its trips contain bus and another mode of transportation.  

The highest number of bicycle trips are gathered in Cluster 4. Notably, this group contains Bike only trips. In contrast, Cluster 3 shows the most variety in bike usage, as trips combining bicycles and other modes of transportation amount to 0.31\%. 

\begin{table}[H]
\centering
\begin{tabular}{||c|c|c|c|c|c|c|c||}
\hline
 & \textbf{C 0} & \textbf{C 1} & \textbf{C 2} & \textbf{C 3} & \textbf{C 4}  & \textbf{C 5} & \textbf{C 6} \\
[0.5ex] 
\hline \hline 
Avg number of trips & 2.34 & 1.94 & 2.28 & 2.27 & 1.78 & \textbf{1.67} & \textbf{2.4} \\
Avg travel time [min] & 35.38 & 35.31 & \textbf{32.39} & 34.97 & 35.98 & \textbf{38.72} & 36.10 \\
Avg travel distance [km] & 14.47 & 14.51 & \textbf{12.92} & 14.10 & 14.62 & \textbf{16.49} & 15.06 \\ 
Avg number of transfer & \textbf{0.21} & \textbf{0.30} & 0.22 & 0.29 & 0.23 & 0.29 & 0.24 \\ 
\hline
Metro - Metro trips [\%] & 41.05 & 32.19 & 38.15 & 31.84 & 38.73 & 43.52 & \textbf{47.94} \\ 
Metro - Bus trips [\%] & 3.56 & 3.03 & 3.80 & 3.57 & 3.44 & 3.16 & 4.62 \\
Metro - Bike trips [\%] & 0.00 & 0.02 & 0.01 & 0.02 & 0.00 & 0.00 & 0.04 \\
Bus - Metro trips [\%] & 3.52 & 3.09 & 3.70 & 3.57 & 3.27 & 3.19 & 4.63 \\ 
Bus - Bus trips [\%] & 51.62 & \textbf{61.38} & 54.02 & 60.65 & 54.14 & 49.74 & 42.57    \\ 
Bus - Bike trips [\%] & 0.00 & 0.00 & 0.00 & 0.01 & 0.00 & 0.00 & 0.00  \\
Bike - Metro trips [\%] & 0.00 & 0.00 & 0.02 & 0.02 & 0.00 & 0.00 & 0.03 \\
Bike - Bus trips [\%] & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.01  \\
Bike - Bike trips [\%] & 0.21 & 0.25 & 0.25 & 0.27 & \textbf{0.37} & 0.35 & 0.13 \\  [1ex]
\hline 
\end{tabular}
\caption{Cluster analysis} %distribution within each cluster
\label{table:clusterAnalysis}
\end{table}


\subsection{Discussion}
This part of the project makes an argument for a better characterization of public transit users. We set the goal to identify distinct traveling behaviors, without restricting these to typical commuting and non-commuting behaviors. As such, we tune the parameter \textit{K} to cluster behaviors beyond binary classification. 

We find seven distinct behaviors in the reduced unlabeled dataset. These are:

\begin{itemize}

\item Cluster 0 : Users who prefer to travel without transfers, or transfers between the same mode of transportation. When these users do transfer, they do so between Metro and Bus only.

\item Cluster 1 : Users with preference for bus as mode of transportation. These users, combined with Cluster 4, are the least related to the "commuters" acquired by labeled data.

\item Cluster 2 : Users performing short trips. These users travel the shortest distances and spend the least amount of time per trip. These users do not show a specific preference for mode of transportation, showing a distributed usage among almost all combinations (except for Bus and Bike combinations).

\item Cluster 3 : Users who complement their trips by bicycle usage. These is the smallest user group. It is also the least related to "Non-commuters".

\item Cluster 4 : Rare public transit users with the lowest average number of trips. When they travel, they have the highest usage of bike only trips. These users, combined with Cluster 1, are the least related to the "commuters" acquired by labeled data.

\item Cluster 5 : This is the largest user group, and it is weakly related to Non-commuters. These users perform the longest longest trips, and travel the largest distances.

\item Cluster 6 : These users are the heaviest public transit users since they perform largest amount of trips. Furthermore, they show a strong preference for Metro only trips. These users are strongly related to "commuters" as labeled by survey data. 

\end{itemize}

Our analysis unravels basic attributes of the seven behaviors. However, we acknowledge that a more detailed statistical analysis could potentially demonstrate each user group's characteristics more in depth. Furthermore, Metropolitan Transportation experts may bring in special domain knowledge to better explain the differences among clusters. 
 

\newpage
\section{Conclusion and future work}
\label{sec:conclusion}
In this project we examine public transit traveling behavior using data mining techniques. As the main contribution of this work, we propose a three dimensional representation that takes advantage of the local properties of weekly schedules, and incorporates up to 26 different trip attributes. Using this representation we compare supervised and unsupervised machine learning techniques for pattern recognition. 

\textbf{Three dimensional representation}

The most significant advantage of the proposed representation is the ability to analyze spatiotemporal features as a unit. This is a result of representing the distribution of each user's trips overtime on a plane, and the spatial attributes as depth. As such, each temporal pixel may contain at most one trip, according to its boarding time. In our work we crop all boarding times to use the hour as a key. However, this may result in overlapping trips if more than one trip started at during the same hour (i.e. Trip 1 starts at 10:01 am, Trip 2 starts at 10:59 am). Possible simple solutions include to concatenate said trips, or create a finer temporal pixel grid. 

However, the main drawback of our representation is its sparsity. The first major implication of this is the growth in computational resources needs, as three dimensional arrays require more storage space and ram requirements than the original raw csv files. In fact, the space requirements became unmanageable while attempting to analyze the whole dataset during the unsupervised task, leading to the decision to use only a reduced dataset. Future work could focus on stronger techniques for parallel computing and multi-threading, and compression, in order to analyze the whole dataset. 

The second major implication is the meaning of zero values in our representation. When constructed, all user cubes are "empty", which is signaled by having zero values in all its cells. However, some trip attributes  may have zero values that do not necessarily mean empty fields. Thus, while considerable effort was put into avoiding zero values (i.e. in the creation of numerical IDs for modes, lines and stops), some attributes still contain them (i.e. start/end hours or number of transfers). Future work could investigate the effect of having a separate special value for empty fields (-1, for example).

\textbf{Dimensionality reduction}

Due to its sparsity, our representation requires compression. As such, we explore two approaches for dimensionality reduction. On the one hand, we perform typical feature selection by scoring each trip attribute according to a variety of techniques. On the other hand, we build and train a convolutional autoencoder that maps the high dimensional user representations to a low dimensional space. 

The fundamental difference between these two approaches is that feature selection operates at a trip level, disregarding attributes that do not aid in identifying the correct user label. Meanwhile, the autoencoder has a more general view of the traveling behavior as it works at a user level. At every train iteration, the autoencoder attempts to deconstruct and reconstruct each user's behavior, without committing each one of them to memory. Therefore, while feature selection compares attributes to each other, the autoencoder compares users to each other, thus abstracting the main similarities and differences among them.

To the best of our knowledge, three dimensional representations and autoencoding techniques have not yet been applied in the domain of Metropolitan Transportation. Therefore, there is still considerable amounts of future work in which this project can be expanded. Some suggestions are the application of three dimensional convolutional filters, using the feature slices as an extra dimension instead of channels. Extensions to the autoencoder are also interesting paths to follow, for example denoising autoencoders and variational autoencoders could be compared to this work's simple implementation.

\textbf{Machine learning for pattern recognition}

Our findings show that ensemble models improve performance over any weak classifier alone. Furthermore, they illustrate that the required complexity of the model has a balance between same-type and different-type learning algorithms. 

Still, the task of binary classification for identifying commuters is ill formed. Studies with high performance models are able to find commuters as labeled by self report surveys. These labels are noisy and do not always represent what a commuter really is, as shown in Section \ref{sec:userCubes}. Furthermore, the labels are limited to the time they were collected, as behavior changes over time.  

Therefore, this work takes a step towards applying unsupervised machine learning techniques and proving its usefulness goes beyond what supervised learning can. Firstly, by using unsupervised techniques, much more data is available for mining. Secondly, the findings are based on the data itself, not on self reported information from users. 

Our findings show that there are 7 types of traveling behaviors in our dataset. These show distinct behaviors, such as preferences for modes of transportation, level of usage of the public transit network, or transfer habits.For a direct comparison, future work could perform clustering with $K = 2$, so we compare the task of binary classification with noisy labels, and biclustering.

\textbf{End to end learning}


As mentioned in Section \ref{sec:representations}, the effectiveness of a representation depends on the task for which it is used. Hence, it is reasonable to believe that if the representations are able to learn according to the task performance, then the representations will improve leading to an improvement in the task itself. This is an example of end to end learning, a principle applied in new popular areas such as Deep Learning for image classification.

Though applied to supervised learning tasks, end to end learning is yet to be studied for unsupervised tasks. While supervised learning has quantitative ways of evaluation (such as objective functions to be optimized), unsupervised learning cannot be evaluated in the same way due to the lack of labels. Thus, the crucial part for this approach is to find an evaluation metric for the clusters that are created. There exists a few pioneer studies that explore this idea. For example, DEC \cite{xie2016unsupervised}, Jule \cite{yang2016joint}, and Depict \cite{dizaji2017deep}, all models for image clustering developed in the last year. 

In the context of this project, if we find a way to evaluate the clusters, taking advantage of the domain knowledge from Transportation specialists, we could be able to propagate the error from the clustering to the autoencoder. 
 
\textbf{Use case relevance}

This project analyzes one month worth of public transit data from the city of Beijing, China. Beijing poses a challenging use case due to its rapid urbanization over the last years. Furthermore, its public transit network is extensive and undergoes constant development in order to serve millions of passengers. As such, public transit users in Beijing present traveling behaviors that are more complex than typical "commuter" definitions can describe. 

Different from most studies on the field, by studying users over a longer timespan (one month compared to one week), we are able to discern these complex behaviors. However, data quality may be sacrificed while intending to grasp a long period of time, since it opens possibilities for data to be faulty. However, data acquisition methods grow as rapidly as the public transit network. 

We hope that in the future we have more complete data. This will extend the possibilities for future work, for example by linking boarding/alighting areas to possible Points of Interest that users are going from/to. Furthermore, combining Semantic Technology (i.e. geospatial ontologies) with the present work could aid in digging deeper into the spatial attributes.of a trip. This way we will be able to have a better understanding of public transit user's behaviors. 


\newpage
\bibliography{mybib}{}
\bibliographystyle{plain}

\end{document}
